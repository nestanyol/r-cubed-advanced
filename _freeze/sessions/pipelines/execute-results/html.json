{
  "hash": "f57851230e38050434216ce95a8b640d",
  "result": {
    "markdown": "---\nexecute: \n  eval: false\n---\n\n\n# Creating automatic analysis pipelines {#sec-pipelines}\n\n\n\n::: callout-warning\nðŸš§ This section is being actively worked on. ðŸš§\n:::\n\n\n\n::: {.callout-note appearance=\"minimal\" collapse=\"true\"}\n## Instructor note\n\nBefore beginning, get them to recall what they remember of the previous\nsession, either with something like Mentimeter or verbally. Preferably\nsomething like Mentimeter because it allows everyone to participate, not\njust the ones who are more comfortable being vocal to the whole group.\n:::\n\nDepending on how much time you've spent working on data analyses, you\nhave probably experienced (many) times where you are working on a\nproject and forget what code needs to be run first, what order other\ncode needs to run in, and what pieces of code need to be re-run in order\nto update other results. Things get confusing quickly, even for fairly\nsimple projects. This probably happens most often when you return to a\nproject after a month or two and completely forget the state of the\nproject and analysis.\n\nThis is where formal data analysis pipeline tools come in and play a\nrole. By setting up your analysis into distinct steps, with clear inputs\nand outputs, and use a system that tracks those inputs and outputs, you\ncan make things a lot easier for yourself and others. This session is\nabout applying tools that make and manage these pipelines.\n\n## Learning objectives\n\nThe overall objective for this session is to:\n\n1.  Identify and apply an approach to create an analysis pipeline that\n    makes your analysis steps, from raw data to finished manuscript,\n    explicitly defined so that updating it by either you or\n    collaborators is as easy as running a single function.\n\nMore specific objectives are to:\n\n1.  Describe the computational meaning of pipeline and how pipelines are\n    often done in research. Explain why a well-designed pipeline can\n    streamline your collaboration, reduce time spent doing an analysis,\n    make your analysis steps explicit and easier to work on, and\n    ultimately contribute to more fully reproducible research.\n2.  Explain the difference between a \"function-oriented\" workflow vs a\n    \"script-oriented\" workflow and why the function-based approach has\n    multiple advantages from a time- and effort-efficiency point of\n    view.\n3.  Use the functions within `{targets}` to apply the concepts of\n    building pipelines in your analysis project.\n4.  Continue applying the concepts and functions used from the previous\n    session.\n\n## Exercise: How do you re-run analyses when something changes?\n\n> Time: \\~14 minutes.\n\nWe've all been in situations where something in our analysis needs to\nchange. Maybe we forgot to remove a certain condition (like unrealistic\nBMI). Or maybe our supervisor suggests something we hadn't considered in\nthe analysis. Or maybe during peer review of our manuscript, a reviewer\nmakes a suggestion that would improve the understanding of the paper.\nWhatever the situation, we inevitably need to re-run our analysis. And\ndepending on what the change was, we might need to run the full analysis\nall over again. So what is your **exact** workflow when you need to\nre-run code and update your results? Assume it's a change somewhere\nearly in the data processing stage.\n\n1.  Take 2 min to think about the workflow you use. Try to think of the\n    *exact* steps you need to take, what *exactly* you do, and how long\n    that usually takes.\n2.  For 8 min, in your group share and discuss what you've thought. How\n    do your experiences compare to each other?\n3.  For the remaining time, we'll briefly share with everyone about what\n    they've thought and discussed.\n\n## What is a data analysis \"pipeline\"?\n\nTODO: Should this be a reading exercise or the instructor talk about it?\n\nA pipeline can be any process where the steps between a start and an end\npoint are very clear, explicit, and concrete. These highly distinct\nsteps can be manual, human involved or can be completely automated by a\nrobot or computer. For instance, in car factories, the pipeline from the\ninput raw materials to the output vehicle are extremely well described\nand implemented. Or like during the pandemic, the pipeline for testing\n(at least in Denmark and several other countries) was highly structured\nand clear for the workers doing the testing and the people having the\ntest done: A person goes in, scans their ID card, has the test done,\nworker inputs the results, results get sent immediately to the health\nagency as well as to the person based on their ID contact information\n(or via a secure app).\n\nHowever, in research, especially around data collection and analysis, we\noften hear or read about \"pipelines\". But looking closer, these aren't\nactual pipelines because the individual steps are not very clear and not\nwell described, often requiring a fair amount of manual human attention\nand intervention. Particularly within computational environments, a\npipeline is when there is minimal to *no* human intervention from raw\ninput to finished output. Why aren't these data \"pipelines\" in research\nactual pipelines? Because:\n\n1.  Anything with data ultimately must be on the computer,\n2.  Anything automatically done on the computer must be done with code,\n3.  Not all researchers write code,\n4.  Researchers who do write code rarely publish and share it,\n5.  Code that is shared or published (either publicly or within the\n    research group) is not written in a way that allows a pipeline to\n    exist,\n6.  And, research is largely non-reproducible [@Trisovic2022;\n    @Seibold2021; @Laurinavichyute2021].\n\nA data analysis pipeline would by definition be a reproducible,\nreadable, and code-based data analysis. We researchers as a group have a\nlong way to go before we can start realistically implementing data\nanalysis pipelines.\n\nThis isn't to diminish the work of researchers, but a basic observation\non the systemic, social, and structural environment surrounding us. We\nas researchers are not trained in writing code, nor do we have a strong\nculture and incentive structure around learning, sharing, reviewing, and\nimproving code. Nor are we often allowed to get (or use) funds to hire\npeople who *are* trained and skilled in programmatic thinking and\nprogramming. Otherwise courses like this wouldn't need to exist :shrug:\n\nSo how would a data analysis pipeline look? Before we get to that\nthough, we have to separate two things: exploratory data analysis and\nfinal paper data analysis. In exploratory data analysis, there will\nlikely be a lot of manual, interactive steps involved that may or may\nnot need to be explicitly stated and included in the analysis plan and\npipeline. But for the final paper and what results would be included, we\ngenerally have some basic first ideas. Let's list a few items that we\nwould want to do before the primary statistical analysis:\n\n1.  A table of some basic descriptive statistics of the study\n    population, such as mean, standard deviation, or counts of basic\n    discrete data (like treatment group).\n2.  A figure visualizing the counts or other statistics for variables of\n    interest that are discrete/categorical. For the data in this course,\n    that would be variables like gender and age.\n3.  A figure showing the distribution of your main variables of\n    interest. In this case, ours are the lipidomic variables.\n4.  The paper with the results included.\n\n\n```{mermaid}\n%%| label: fig-pipeline-schematic\n%%| fig-cap: Very simple flow chart of steps in an analysis pipeline.\n%%| echo: false\n%%| eval: true\n%%{init:{'theme':'forest', 'flowchart':{'nodeSpacing': 20, 'rankSpacing':30}}}%%\ngraph LR\n    data[Data] --> fn_desc{{function}} \n    fn_desc --> tab_desc[Table:<br>descriptive<br>statistics]\n    data --> fn_plot{{function}}\n    fn_plot{{function}} --> plot_discr[Plot:<br>Discrete<br>variables]\n    tab_desc --> paper[Paper]\n    plot_discr --> paper\n    data --> fn_plot_vars{{function}}\n    fn_plot_vars{{function}} --> plot_distrib[Plot:<br>Continuous<br>variables]\n    plot_distrib --> paper\n\nlinkStyle 0,1,2,3,4,5,6,7,8 stroke-width:1px;\n```\n\n\nNow that we conceptually have drawn out the sets of tasks to complete in\nour pipeline, we can start using R to build it.\n\n## Using targets to manage the pipeline\n\nThere are a few packages to help build pipelines in R, but the most\ncommonly used, well-designed, and maintained one is called `{targets}`.\nWith this package, you specify outputs you want to create and\n`{targets}` will track them for you. So it will know which output\ndepends on which other one and which ones need to be updated.\n\n::: {.callout-note appearance=\"minimal\" collapse=\"true\"}\n## Instructor note\n\nAsk participants, which do they think it is: a build dependency or a\nworkflow dependency. Because it is directly used to run analyses and\nprocess the data, it would be a build dependency.\n:::\n\nFirst, since we are using `{renv}`, we need to install `{targets}` in\nthe project environment. And since `{targets}` is a build dependency, we\nadd it to the `DESCRIPTION` file with:\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nuse_package(\"targets\")\n```\n:::\n\n\nNow that it's added to the project R library, let's set up our project\nto start using it!\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntargets::use_targets()\n```\n:::\n\n\nThis will add several files:\n\n    .\n    â”œâ”€â”€ _targets.R\n    â”œâ”€â”€ run.R\n    â””â”€â”€ run.sh\n\n\n::: {.cell}\n\n:::\n\n\nThe most important file is the `_targets.R` file. The other two files\nare used for other situations (like running on a Linux server) that we\nwon't cover in the course. Before we continue though, let's **commit\nthese new files to the Git history**.\n\n::: {.callout-note appearance=\"minimal\" collapse=\"true\"}\n## Instructor note\n\nLet them read it before going over it again to reinforce\nfunction-oriented workflows and how `{targets}` and the `tar_target()`\nworks.\n:::\n\n::: callout-info\n## Reading task: \\~8\n\nNext, open up the `_targets.R` and we'll take a look at what it\ncontains.\n\n::: {.callout-note collapse=\"true\"}\n## Click this to see the file contents\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.sourceCode}\n# Created by use_targets().\n# Follow the comments below to fill in this target script.\n# Then follow the manual to check and run the pipeline:\n#   https://books.ropensci.org/targets/walkthrough.html#inspect-the-pipeline # nolint\n\n# Load packages required to define the pipeline:\nlibrary(targets)\n# library(tarchetypes) # Load other packages as needed. # nolint\n\n# Set target options:\ntar_option_set(\n  packages = c(\"tibble\"), # packages that your targets need to run\n  format = \"rds\" # default storage format\n  # Set other options as needed.\n)\n\n# tar_make_clustermq() configuration (okay to leave alone):\nCLUSTERMQ\n\n# tar_make_future() configuration (okay to leave alone):\nFUTURE\n\n# Run the R scripts in the R/ folder with your custom functions:\ntar_source()\n# source(\"other_functions.R\") # Source other scripts as needed. # nolint\n\n# Replace the target list below with your own:\nlist(\n  tar_target(\n    name = data,\n    command = tibble(x = rnorm(100), y = rnorm(100))\n#   format = \"feather\" # efficient storage of large data frames # nolint\n  ),\n  tar_target(\n    name = model,\n    command = coefficients(lm(y ~ x, data = data))\n  )\n)\n```\n:::\n:::\n\n:::\n\nOne small thing to notice are the `#nolint` comments used throughout.\nThese \"tags\" are used to tell `{lintr}` to ignore these lines when\nrunning the lintr.\n\nNext, notice `tar_target()` function used at the end of the script.\nThere are two main arguments for it `name` and `command`. The way that\n`{targets}` works is similar to how you'd write R code to assign the\noutput of a function to an object.\n\n``` r\nobject_name <- function_in_command(input_arguments)\n```\n\nIs the same as:\n\n``` r\ntar_target(\n  name = object_name,\n  command = function_in_command(input_arguments)\n)\n```\n\nWhat this means is that `{targets}` follows a [\"function-oriented\"\nworkflow](https://books.ropensci.org/targets/functions.html#problems-with-script-based-workflows),\nnot a \"script-based\" workflow. What's the difference? In a\nscript-oriented workflow, each R file is run in a specific order, so you\nmight end up with an R file that has code like:\n\n``` r\nsource(\"R/1-process-data.R\")\nsource(\"R/2-basic-statistics.R\")\nsource(\"R/3-create-plots.R\")\nsource(\"R/4-linear-regression.R\")\n```\n\nWhile in a function-oriented workflow, it might look more like:\n\n``` r\nsource(\"R/functions.R\")\nraw_data <- load_raw_data(\"file/path/data.csv\")\nprocessed_data <- process_data(raw_data)\nbasic_stats <- calculate_basic_statistics(processed_data)\nsimple_plot <- create_plot(processed_data)\nmodel_results <- run_linear_reg(processed_data)\n```\n\nWith this workflow, each function takes an input dataset and contains\nall the code to create the results into one output, like a figure in a\npaper. If you've taken the [intermediate R\ncourse](https://r-cubed-intermediate.rostools.org/), you'll notice that\nthis function-oriented workflow is the workflow we covered. There are so\nmany advantages to this type of workflow and is the reason many powerful\nR packages are designed around making use of this type workflow.\n\nIf we take these same code and convert it into the `{targets}` format,\nthe end of `_targets.R` file would like this:\n\n``` r\nlist(\n  tar_target(\n    name = raw_data,\n    command = load_raw_data(\"file/path/data.csv\")\n  ),\n  tar_target(\n    name = processed_data,\n    command = process_data(raw_data)\n  ),\n  tar_target(\n    name = basic_stats,\n    command = calculate_basic_statistics(processed_data)\n  ),\n  tar_target(\n    name = simple_plot,\n    command = create_plot(processed_data)\n  ),\n  tar_target(\n    name = model_results,\n    command = run_linear_reg(processed_data)\n  )\n)\n```\n:::\n\nLet's start writing code to create the four items we listed above: some\ndescriptive statistics, a plot of some discrete data, a plot of the\ncontinuous lipid variables, and a report (R Markdown). Since we'll use\n`{tidyverse}`, specifically `{dplyr}`, to calculate the summary\nstatistics, we need to add it to our dependencies and install it in the\nproject library. `{tidyverse}` is a special \"meta\"-package so we need to\nadd it to the `\"depends\"` section of the `DESCRIPTION` file.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nuse_package(\"tidyverse\", \"depends\")\nuse_package(\"dplyr\")\n```\n:::\n\n\nCommit the changes made to the `renv.lock` and `DESCRIPTION` file in the\nGit history. Open up the `doc/lesson.Rmd` file and create a new header\nand code chunk at the bottom of the file.\n\n    ## Basic statistics\n\n    ```{{r setup}}\n    library(tidyverse)\n    source(here::here(\"R/functions.R\"))\n    load(here::here(\"data/lipidomics.rda\"))\n    ```\n\n    ```{{r basic-stats}}\n\n    ```\n\n\n::: {.cell}\n\n:::\n\n\nWe want to calculate the mean and SD for each metabolite and then, to\nmake it more readable, to round the numbers to one digit. We covered\nthis in the\n[functionals](https://r-cubed-intermediate.rostools.org/dry-functionals.html#summarizing-long-data-like-the-rr-dataset)\nsession of the intermediate course, so we will apply these same\nprinciples and code here. To do that, we need to use `group_by()` on the\n`metabolites`, use `across()` inside `summarise()` so we can give it the\n`mean()` and `sd()` functions, followed by `mutate()`ing each numeric\ncolumn (`across(where(is.numeric))`) to `round()` to 1 digit. Let's\nwrite out the code!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlipidomics %>%\n  group_by(metabolite) %>%\n  summarise(across(value, list(mean = mean, sd = sd))) %>%\n  mutate(across(where(is.numeric), round, digits = 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.sourceCode}\n# A tibble: 12 Ã— 3\n   metabolite               value_mean value_sd\n   <chr>                         <dbl>    <dbl>\n 1 CDCl3 (solvent)               180       67  \n 2 Cholesterol                    18.6     11.4\n 3 FA -CH2CH2COO-                 33.6      7.8\n 4 Lipid -CH2-                   537.      61.9\n 5 Lipid CH3- 1                   98.3     73.8\n 6 Lipid CH3- 2                  168.      29.2\n 7 MUFA+PUFA                      32.9     16.1\n 8 Phosphatidycholine             31.7     20.5\n 9 Phosphatidylethanolamine       10        7.6\n10 Phospholipids                   2.7      2.6\n11 PUFA                           30       24.1\n12 TMS (interntal standard)      123      130. \n```\n:::\n:::\n\n\nAfter that, run `{styler}` (`Ctrl-Shift-P`, then type \"style file\") on the file. Than we will **commit**\nthe changes to the Git history.\n\n## Exercise: Convert summary statistics code into a function\n\n> Time: \\~25 minutes.\n\nWhile inside the `doc/lesson.Rmd` file, use the \"function-oriented\"\nworkflow, as taught in the [intermediate\ncourse](https://r-cubed-intermediate.rostools.org/dry-functions.html#the-basics-of-a-function),\nto take the code we wrote above and convert it into a function. Complete\nthese tasks:\n\n1.  Wrap the code with `function() {...}` and name the new function\n    `descriptive_stats`.\n2.  Replace `lipidomics` with `data` and put `data` as an argument\n    inside the brackets of `function()`.\n3.  Add `dplyr::` to the start of each `{dplyr}` function used inside\n    your function (except for `where()`, which comes from the\n    `{tidyselect}` package).\n4.  Run `{styler}` (`Ctrl-Shift-P`, then type \"style file\") and `lintr::lint_dir()` in the R Console on the code to make\n    sure it is formatted correctly. You might need to manually force a\n    styling if lines are too long.\n5.  With the *cursor* inside the function, add some roxygen\n    documentation with `Ctrl-Shift-P` followed by typing \"roxygen\n    comment\". Remove the lines that contain `@examples` and `@export`,\n    then fill in the other details (like the `@params` and `Title`). In\n    the `@return` section, write \"A data.frame/tibble.\"\n6.  Cut and paste the function over into the `R/functions.R` file.\n7.  Source the `R/functions.R` file (`Ctrl-Shift-S`) and then test the\n    code by running `descriptive_stats(lipidomics)` in the Console. If\n    it works, do the last task.\n8.  Save both files and then open the Git interface and commit the\n    changes you made to them.\n\n::: callout-info\nIn the intermediate course, we highly suggested using `return()` at the\nend of the function. Technically we don't need an explicit `return()`,\nsince the output of the last code that R runs within the function will\nbe the output of the function. This is called an \"implicit return\" and\nwe will be using this feature throughout the rest of this course.\n:::\n\nHere is some scaffolding to help you get started:\n\n``` r\ndescriptive_stats <- function(___) {\n  ___\n}\n```\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"**Click for a potential solution**. Only click if you are struggling or are out of time.\"}\n#' Calculate descriptive statistics of each metabolite.\n#'\n#' @param data Lipidomics dataset.\n#'\n#' @return A data.frame/tibble.\n#'\ndescriptive_stats <- function(data) {\n  data %>%\n    dplyr::group_by(metabolite) %>%\n    dplyr::summarise(dplyr::across(value, list(mean = mean, sd = sd))) %>%\n    dplyr::mutate(dplyr::across(tidyselect::where(is.numeric), round, digits = 1))\n}\n```\n:::\n\n\n## Adding a step in the pipeline\n\nNow that we've created a function to calculate some basic statistics, we\ncan now add it as a step in the `{targets}` pipeline. Open up the\n`_targets.R` file and go to the end of the file, where the `list()` and\n`tar_target()` code are found. In the first `tar_target()`, replace the\ntarget to load the lipidomic data. In the second, replace it with the\n`descriptive_stats()` function. If we want to make it easier to remember\nwhat the target output is, we can add `df_` to remind us that it is a\ndata frame. It should look like:\n\n``` r\nlist(\n  tar_target(\n    name = lipidomics,\n    command = load(here::here(\"data/lipidomics.rda\"))\n  ),\n  tar_target(\n    name = df_stats_by_metabolite,\n    command = descriptive_stats(lipidomics)\n  )\n)\n```\n\nLet's run `{targets}` to see what happens! You can either use the\nCommand Palette (`Ctrl-Shift-P`, then type \"run targets\") or run this\ncode in the Console:\n\n``` r\ntargets::tar_make()\n```\n\nIt won't run though. That's because `{targets}` does not work when being\ngiven data in a `.rda` format. Instead `{targets}` want a \"raw\" data\nfile that itself can store as `.rda`. To account for this we need to\nalter our data wrangling script. From here we will open up the the\n`data-raw/nmr-omics.r` script and replace:\n\n``` r\nusethis::use_data(lipidomics, overwrite = TRUE)\n```\n\nwith\n\n``` r\nreadr::write_csv(lipidomics, here::here(\"data/lipidomics.csv\"))\n```\n\n\n::: {.cell}\n\n:::\n\n\nWe haven't added `{readr}` as a dependency yet, so let's add it to the\nDESCRIPTION file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuse_package(\"readr\")\n```\n:::\n\n\nWe can now source the script (to generate a new data file), delete the\nold `data/lipidomics.rda` file, and update the `_targets.R` file. First,\nlet's tell Git to ignore this new data file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuse_git_ignore(\"data/lipidomics.csv\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nBecause of the way `{targets}` works, we need to first create a pipeline\ntarget of *only* the path to the data file, and then we can load it with\n`{readr}`.\n\n``` r\nlist(\n    tar_target(\n        name = file,\n        command = \"data/lipidomics.csv\",\n        format = \"file\"\n    ),\n    tar_target(\n        name = lipidomics,\n        command = readr::read_csv(file, show_col_types = FALSE)\n    ),\n    tar_target(\n        name = df_stats_by_metabolite,\n        command = descriptive_stats(lipidomics)\n    )\n)\n```\n\n\n::: {.cell}\n\n:::\n\n\nNow, let's try running `{targets}` again! You can either use the Command\nPalette (`Ctrl-Shift-P`, then type \"run targets\") or run this code in\nthe Console:\n\n``` r\ntargets::tar_make()\n```\n\nIt probably won't run though. That's because `{targets}` doesn't know\nabout the packages that you need for the pipeline. To add it, we need to\ngo to the `tar_option_set()` section of the `_targets.R` file and\nreplace the `packages = c(\"tibble\")` with all the packages we use.\nInstead of typing each package out, we can paste them directly into the\nscript using code, since we already track the packages in the\n`DESCRIPTION` file. To access the dependencies, we can use this\nfunction:\n\n``` r\ndesc::desc_get_deps()$package\n```\n\nBut this code also picks up the dependency on R itself. Usually the R\ndependency is listed as the first item. If it isn't, we will run:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuse_tidy_description()\n```\n:::\n\n\nThis function fixes up the `DESCRIPTION` file, with R now being the\nfirst item. Using the same code as above from the `{desc}` package, we\ncan remove R with`[-1]` and put it in the place of the `packages`\nargument in the `_targets.R` file:\n\n``` r\npackages = desc::desc_get_deps()$package[-1]\n```\n\n\n::: {.cell}\n\n:::\n\n\nTry running `{targets}` again with either `targets::tar_make()` or the\nCommand Palette (`Ctrl-Shift-P`, then type \"run targets\"). It should run\nthrough! We also see that a new folder has been created called\n`_targets/`. Since we don't want to track this in Git, we should ignore\nit with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuse_git_ignore(\"_targets\")\n```\n:::\n\n\nWe can visualize our individual pipeline targets that we track through\n`tar_target()` now too, which can be useful as you add more and more\ntargets. We will (likely) need to install an extra package (done\nautomatically):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntargets::tar_visnetwork()\n```\n:::\n\n\nOr to see what pipeline targets are outdated:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntargets::tar_outdated()\n```\n:::\n\n\nBefore continuing, let's **commit the changes to the Git history**.\n\n\n::: {.cell}\n\n:::\n\n\n## Exercise: Examine the contents of the `_targets/` folder\n\n> Time: \\~5 minutes.\n\nLike you did with `{renv}`, go into the `_targets/` folder and take a\nlook around. There are two important ones: `meta/` and `objects/`. Look\nin each to get familiar with the files there.\n\n-   The `meta/meta` file contains the metadata about the individual\n    pipeline targets that you add with `tar_target()`.\n-   The `objects/` folder contains the output of each `tar_target()`\n    (e.g. the data from `load()` or the statistics from\n    `descriptive_stats()`).\n\nTODO: Need to run on my end to know what to look for.\n\n## Exercise: Update function to also calculate median and IQR\n\n> Time: \\~8 minutes.\n\nLet's make a change to our function and test out how the\n`tar_outdated()` and `tar_visnetwork()` work.\n\n1.  Open up the `R/functions.R` file.\n2.  Add median and interquartile range (IQR) to the `summarise()`\n    function, by adding it to the end of `list(mean = mean, sd = sd)`,\n    after the second `sd`. Note, IQR should look like `iqr = IQR` since\n    we want the output columns to have a lowercase for the column names.\n3.  Run `tar_outdated()` and `tar_visnetwork()` in the Console (or by\n    using the Command Palette `Ctrl-Shift-P`, then \"targets outdated\" or\n    \"targets visualize\"). What does it show?\n4.  Run `{styler}` (`Ctrl-Shift-P`, then type \"style file\") and `lintr::lint_dir()` in the R Console. You might need to\n    force a reformat if the code is too long by highlighting the line\n    and using `Ctrl-Shift-P`, then \"reformat\".\n5.  Run `tar_make()` in the Console (or `Ctrl-Shift-P`, then \"targets\n    run\", selecting the \"background\" option). Re-check for outdated\n    targets and visualize the network again.\n6.  Open up the Git interface and commit the changes to the Git history.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"**Click for a potential solution**. Only click if you are struggling or are out of time.\"}\n#' Calculate descriptive statistics of each metabolite.\n#'\n#' @param data Lipidomics dataset.\n#'\n#' @return A data.frame/tibble.\n#'\ndescriptive_stats <- function(data) {\n  data %>%\n    dplyr::group_by(metabolite) %>%\n    dplyr::summarise(dplyr::across(value, list(\n      mean = mean,\n      sd = sd,\n      median = median,\n      iqr = IQR\n    ))) %>%\n    dplyr::mutate(dplyr::across(dplyr::where(is.numeric), round, digits = 1))\n}\n```\n:::\n\n\n## Saving intermediate analysis outputs\n\nSometimes you may want a pipeline target to save the output to a file so\nyou can save it to the Git history, like saving a figure, rather than\nstore the result in the `_targets/` folder. To save outputs of a target\noutside of the `_targets/` folder, there are a few specific things we\nhave to do.\n\nFirst, let's write some code to create the plots for our items 2 and 3\nthat we initially decided at the start of this session. Since we're\nusing `{ggplot2}` to write this code, let's add it to our `DESCRIPTION`\nfile and to our `{renv}` project library.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nuse_package(\"ggplot2\")\n```\n:::\n\n\nNext, we'll write the code to create item 2: The bar plot of the counts\nof `gender` and `class`. We'll keep it simple for now. Because the data\nis structured in a long format, we need to trim it down to only the\nunique cases of `code`, `gender`, and `class` with `distinct()`. Then we\ncan make the bar plot. We'll use `position = \"dodge\"` for the bars to be\nside by side, rather than stacked.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngender_by_class_plot <- lipidomics %>%\n  distinct(code, gender, class) %>%\n  ggplot(aes(x = class, fill = gender)) +\n  geom_bar(position = \"dodge\")\ngender_by_class_plot\n```\n\n::: {.cell-output-display}\n![](pipelines_files/figure-html/bar-plot-gender-class-1.png){width=672}\n:::\n:::\n\n\nSince we want to save this plot to a file, we'll use `ggsave()` to save\nit to a new folder called `images/`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggsave(here::here(\"images/gender-by-class.png\"), gender_by_class_plot)\n```\n:::\n\n\nNext, let's do item 3, the distribution of each metabolite. Here we'll\nuse `geom_histogram()`, nothing too fancy. And since the data is already\nin long format, we can easily use `facet_wrap()` to create a plot for\neach metabolite. Like with the plot above, let's save it to a file. We\nuse `scales = \"free\"` because each metabolite doesn't have the same\nrange of values (some are small, others are quite large).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetabolite_distribution_plot <- ggplot(lipidomics, aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(vars(metabolite), scales = \"free\")\nmetabolite_distribution_plot\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](pipelines_files/figure-html/histogram-metabolites-1.png){width=672}\n:::\n:::\n\n\nAnd then save it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggsave(\n  here::here(\"images/metabolite-distribution.png\"),\n  metabolite_distribution_plot\n)\n```\n:::\n\n\nWe now have the basic code to convert over into functions. There are\nsome special things `{targets}` needs in order for it to recognize that\nyou want to save the output as a file. Before we do that though, we'll\ncreate the functions to start with.\n\n## Exercise: Convert the plot code to a function\n\n> Time: \\~20 minutes.\n\nFor now, we will only take the code to make the bar plot and convert it\ninto a function. Just like you did with the `descriptive_stats()`\nfunction in the exercise above, complete these tasks:\n\n1.  Wrap the code with `function() {...}` and name the new function\n    `plot_count_stats`.\n2.  Replace `lipidomics` with `data` and put `data` as an argument\n    inside the brackets of `function()`.\n3.  Replace the `\"images/gender-by-class.png\"` with `image_path` and put\n    `image_path` as an argument inside the brackets of `function()`.\n4.  Add `ggplot2::` and `dplyr::` to the start of each `{ggplot2}` and\n    `{dplyr}` function used inside your function.\n5.  Run `{styler}` (`Ctrl-Shift-P`, then type \"style file\") and `lintr::lint_dir()` in the R Console on the code to make\n    sure it is formatted correctly. You might need to manually force a\n    styling if lines are too long.\n6.  With the *cursor* inside the function, add some roxygen\n    documentation with `Ctrl-Shift-P` followed by typing \"roxygen\n    comment\". Remove the lines that contain `@examples` and `@export`,\n    then fill in the other details (like the `@params` and `Title`). In\n    the `@return` section, write \"A plot object or a file path.\"\n7.  Cut and paste the function over into the `R/functions.R` file.\n8.  Source the `R/functions.R` file (`Ctrl-Shift-S`) and then test the\n    code by running `plot_count_stats(lipidomics)` in the Console. If it\n    works, do the last task.\n9.  Save both files and then open the Git interface and commit the\n    changes you made to them.\n10. Repeat these tasks for the code for the metabolite distribution\n    plot. Call this new function `plot_distributions`.\n\nUse this scaffolding code to help guide you to write the code into a\nfunction.\n\n``` r\nplot_count_stats <- function(___, ___) {\n  plot_output <- ___\n  \n  ggplot2::ggsave(\n    plot = plot_output, \n    filename = here::here(___)\n  )\n  plot_output\n}\n\nplot_distributions <- function(___, ___) {\n  plot_output <- ___\n  \n  ggplot2::ggsave(\n    plot = plot_output, \n    filename = here::here(___)\n  )\n  plot_output\n}\n```\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"**Click for the solution**. Only click if you are struggling or are out of time.\"}\n## This should be in the R/functions.R file.\n#' Plot for basic count data.\n#'\n#' @param data The lipidomics dataset.\n#' @param image_path The file path location where to save the image.\n#'\n#' @return A ggplot2 graph.\n#'\nplot_count_stats <- function(data, image_path) {\n  plot_output <- data %>%\n    dplyr::distinct(code, gender, class) %>%\n    ggplot2::ggplot(ggplot2::aes(x = class, fill = gender)) +\n    ggplot2::geom_bar(position = \"dodge\")\n  ggplot2::ggsave(\n    plot = plot_output,\n    filename = here::here(image_path)\n  )\n  plot_output\n}\n\n#' Plot for basic distribution of metabolite data.\n#'\n#' @param data The lipidomics dataset.\n#' @param image_path The file path location where to save the image.\n#'\n#' @return A ggplot2 graph.\n#'\nplot_distributions <- function(data, image_path) {\n  plot_output <- data %>% \n    ggplot2::ggplot(ggplot2::aes(x = value)) +\n    ggplot2::geom_histogram() +\n    ggplot2::facet_wrap(ggplot2::vars(metabolite), scales = \"free\")\n  ggplot2::ggsave(\n    plot = plot_output,\n    filename = here::here(image_path)\n  )\n  plot_output\n}\n```\n:::\n\n\n## Informing targets of the saved output files\n\nWe now have the functions that we can put into the `_targets.R` file.\nNow we need to change the functions slightly in order for `{targets}` to\nknow about the saved images. When we use the functions inside\n`tar_target()`, we need to do two things: have the function output the\nfile path of the image rather than output the plot; and, set the\nargument `format` in `tar_target()` to `\"file\"`.\n\nTo do the first, a simple fix would be to go into the plot functions and\nreplace the `plot_output` at the end with `image_path`. But! That would\nmean that when we are testing out the function, we couldn't ever\nactually *see* the plot being created. Instead, we can create an\n`if () ... else()` condition to output the plot when we are working\n`interactive()`ly, otherwise output the image path when `{targets}` runs\nit.\n\nSince we'll be using this code at least twice, let's create another\nfunction to do this switch for us. Open up the `R/functions.R` file and\nstart typing out the function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noutput_switch_for_targets <- function(output, path) {\n  if (interactive()) {\n    output\n  } else {\n    path\n  }\n}\n```\n:::\n\n\nAfter we've written it, we'll add some Roxygen documentation\n(`Ctrl-Shift-P`, then type \"roxygen comment\"):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Switch output based on if it is interactive, otherwise it is a file path.\n#'\n#' @param output Output object to send to screen when running interactively.\n#' @param path The file path to where the intermediate object is saved.\n#'\n#' @return An object or a file path.\n#'\noutput_switch_for_targets <- function(output, path) {\n  if (interactive()) {\n    output\n  } else {\n    path\n  }\n}\n```\n:::\n\n\nThen, we go into the two plot functions and replace `plot_output` at the\nend with:\n\n``` r\noutput_switch_for_targets(plot_output, image_path)\n```\n\nTODO: Check purl'ed code to see which line to replace with.\n\nNow, let's add the two functions to the `_targets.R` file, including\nadding the `format = \"file\"` argument. Let's write these two\n`tar_target()` items within the `list()` inside `_targets.R`. To make it\neasier to track things, keep the file name and target `name` the same\n(except maybe add `fig_` at the start of the name).\n\n``` r\nlist(\n  ...,\n  tar_target(\n    name = fig_gender_by_class,\n    command = plot_count_stats(\n      lipidomics, \n      \"images/gender-by-class.png\"\n    ),\n    format = \"file\"\n  ),\n  tar_target(\n    name = fig_metabolite_distribution,\n    command = plot_distributions(\n      lipidomics, \n      \"images/metabolite-distribution.png\"\n    ),\n    format = \"file\"\n  )\n)\n```\n\nNext, test that it works by running `targets::tar_visnetwork()` (or\n`Ctrl-Shift-P`, then type \"target visual\") or `targets::tar_outdated()`.\nYou should see that the new items are \"outdated\". Then run\n`targets::tar_make()` (`Ctrl-Shift-P`, then \"targets run\") to update the\npipeline. If it all works, than **commit the changes to the Git\nhistory**.\n\n\n::: {.cell}\n\n:::\n\n\n## Incorporating targets with R Markdown\n\nLast, but not least, we want to make our final item: The R Markdown\ndocument. Adding an R Markdown document as a target inside `_targets.R`\nis fairly straightforward. We need to install the helper package\n`{tarchetypes}` first:\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nuse_package(\"tarchetypes\")\n```\n:::\n\n\nThen, inside `_targets.R`, uncomment the line where\n`library(tarchetypes)` is commented out. The function we need to use to\nbuild the R Markdown file is `tar_render()`, which needs two things: The\n`name`, like `tar_target()` needs, and the file path to the R Markdown\nfile. Again, like the other `tar_target()` items, add it to the end of\nthe `list()`. Since we're using the `doc/lesson.Rmd` as a sandbox, we\nwon't include it as a pipeline target. Instead we will use the\n`doc/report.Rmd` file:\n\n``` r\nlist(\n  ...,\n  tar_render(\n    name = report_rmd, \n    path = \"doc/report.Rmd\"\n  )\n)\n```\n\nNow when we run `targets::tar_make()` (`Ctrl-Shift-P`, then type \"targets run\"), the R Markdown file also gets\nre-built. But when we use `targets::tar_visnetwork()`, we don't see the\nconnections with plots and descriptive statistics. That's because we\nhaven't used them in a way `{targets}` can recognize. For that, we need\nto use the function `targets::tar_read()`.\n\nTODO: Confirm that we don't see this.\n\nLet's open up the `doc/report.Rmd` file, add a `setup` code chunk below\nthe YAML header, and create a new header and code chunk and make use of\nthe `targets::tar_read()`\n\nTODO: Confirm this works.\n\n    ```{{r setup}}\n    library(tidyverse)\n    library(targets)\n    lipidomics <- tar_read(lipidomics)\n    ```\n\n    ## Results\n\n    ```{{r}}\n    tar_read(df_stats_by_metabolite)\n    ```\n\n    ```{{r}}\n    knitr::include_graphics(tar_read(fig_gender_by_class))\n    ```\n\n    ```{{r}}\n    knitr::include_graphics(targets::tar_read(fig_distribution_metabolites))\n    ```\n\nTODO: Purl this markdown text\n\nWith `targets::tar_read()`, we can access all the stored target items\nusing syntax like we would with `{dplyr}`, without quotes. For the\n`df_stats_by_metabolite`, we can do some minor wrangling with `mutate()`\nand `glue::glue()`, and than pipe it to `knitr::kable()` to create a\ntable in the output document. The `{glue}` package is really handy for\nformatting text based on columns. If you use `{}` inside a quoted\nstring, you can use columns from a data frame, like `value_mean`. So we\ncan use it to format the final table text to be `mean value (SD value)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntargets::tar_read(df_stats_by_metabolite) %>% \n  mutate(MeanSD = glue::glue(\"{value_mean} ({value_sd})\")) %>%\n  select(Metabolite = metabolite, `Mean SD` = MeanSD) %>%\n  knitr::kable(caption = \"Descriptive statistics of the metabolites.\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Descriptive statistics of the metabolites.\n\n|Metabolite               |Mean SD      |\n|:------------------------|:------------|\n|CDCl3 (solvent)          |180 (67)     |\n|Cholesterol              |18.6 (11.4)  |\n|FA -CH2CH2COO-           |33.6 (7.8)   |\n|Lipid -CH2-              |536.6 (61.9) |\n|Lipid CH3- 1             |98.3 (73.8)  |\n|Lipid CH3- 2             |168.2 (29.2) |\n|MUFA+PUFA                |32.9 (16.1)  |\n|Phosphatidycholine       |31.7 (20.5)  |\n|Phosphatidylethanolamine |10 (7.6)     |\n|Phospholipids            |2.7 (2.6)    |\n|PUFA                     |30 (24.1)    |\n|TMS (interntal standard) |123 (130.4)  |\n:::\n:::\n\n\nRe-run `targets::tar_visnetwork()` (`Ctrl-Shift-P`, then type \"targets visual\") to see that it now detects the connections\nbetween the pipeline targets. Now, run `targets::tar_make()` (`Ctrl-Shift-P`, then type \"targets run\") again to\nsee everything re-build! Last things are to re-style using\n`{styler}` (`Ctrl-Shift-P`, then type \"style file\"), then **commit** the changes to the Git history\nbefore moving on.\n\n## Exercise: Removing the stored pipeline data\n\n> Time: \\~15 minutes.\n\nLike with `{renv}`, sometimes you need to start from the beginning and\nclean everything up because there's an issue that you can't seem to fix.\nIn this case, `{targets}` has a few functions to help out. Here are four\nthat you can use to delete stuff (also [described on the targets\nbook](https://books.ropensci.org/targets/data.html#cleaning-up-local-internal-data-files)):\n\n`tar_invalidate()`\n\n:   This removes the metadata on the target in the pipeline, but doesn't\n    remove the object itself (which `tar_delete()` does). This will tell\n    `{targets}` that the target is out of date, since it has been\n    removed, even though the data object itself isn't present. You can\n    use this like you would `select()`, by naming the objects directly\n    or using the `{tidyselect}` helpers (e.g. `everything()`,\n    `starts_with()`).\n\n`tar_delete()`\n\n:   This deletes the stored objects (e.g. the `lipidomics` or\n    `df_stats_by_metabolite`) inside `_targets/`, but does not delete\n    the record in the pipeline. So `{targets}` will see that the\n    pipeline doesn't need to be rebuilt. This is useful if you want to\n    remove some data because it takes up a lot of space, or, in the case\n    of GDPR and privacy rules, you don't want to store any sensitive\n    personal health data in your project. Use it like\n    `tar_invalidate()`, with functions like `everything()` or\n    `starts_with()`.\n\n`tar_prune()`\n\n:   This function is useful to help clean up left over or unused objects\n    in the `_targets/` folder. You will probably not use this function\n    too often.\n\n`tar_destroy()`\n\n:   The most destructive, and probably more commonly used, function.\n    This will delete the entire `_targets/` folder for those times when\n    you want to start over and re-run the entire pipeline again.\n\n1.  Try out `tar_invalidate()` by running these code in the Console, one\n    at a time, and see what output they give:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    tar_read(lipidomics)\n    tar_invalidate(everything())\n    tar_read(lipidomics)\n    tar_outdated()\n    tar_make()\n    tar_outdated()\n    ```\n    :::\n\n\n2.  Try out `tar_delete()` by also running these code in the Console,\n    one at a time, and see what output they give:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    tar_read(lipidomics)\n    tar_delete(everything())\n    tar_read(lipidomics)\n    tar_outdated()\n    tar_make()\n    tar_outdated()\n    ```\n    :::\n\n\nWhile this isn't an issue for our data (since it is an open dataset),\nit's good practice to not store the original, individual level data in\nthe `_targets/` folder for those of us working in more strict\nenvironments when it comes to personal health data.\n\nOpen the `_targets.R` file and write this code at the end of the file:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntar_delete(lipidomics)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nBefore ending, open the Git interface and commit the changes you made.\nThen push your changes up to GitHub.\n\n## Summary\n\n-   Use a function-oriented workflow together with `{targets}` to build\n    your data analysis pipeline and track your \"pipeline targets\".\n-   List individual \"pipeline targets\" using `tar_target()` within the\n    `_targets.R` file.\n-   Visualize target items in your pipeline with\n    `targets::tar_visnetwork()` or list outdated items with\n    `targets::tar_outdated()`.\n-   When saving output as a file, ensure that your function outputs the\n    path to the file, *not* the output itself. Then use\n    `format = \"file\"` as an argument in `tar_target()`.\n-   Within R Markdown files, use `targets::tar_read()` to access saved\n    pipeline outputs. To include the R Markdown in the pipeline, use\n    `{tarchetypes}` and the function `tar_render()`.\n-   Delete stored pipeline output with `tar_delete()`.\n",
    "supporting": [
      "pipelines_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}