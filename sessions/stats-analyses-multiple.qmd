---
execute: 
  freeze: auto
---

# Efficiently running many analyses at once {#sec-stats-analyses-multiple}

{{< include ../includes/_wip.qmd >}}

```{r setup}
#| include: false
source(here::here("R/functions.R"))
extract_functions_from_qmd()
source(here::here("R/project-functions.R"))
library(tidyverse)
library(tidymodels)
load(here::here("data/lipidomics.rda"))
```

::: {.callout-note appearance="minimal" collapse="true"}
## Instructor note

Before beginning, get them to recall what they remember of the previous
session, either with something like Mentimeter or verbally. Preferably
something like Mentimeter because it allows everyone to participate, not
just the ones who are more comfortable being vocal to the whole group.
:::

TODO: intro here

## Learning objectives

The overall objective for this session is to:

1.  Describe the basic framework underlying most statistical analyses
    and use R to generate statistical results using this framework.

More specific objectives are to:

1.  Recall principles of functional programming and apply them to
    running statistical analyses by using the `{purrr}` package.
2.  Describe what a resampling technique is, the types available, and
    why it can help estimate the variability of model results. Apply
    functions from `{rsample}` and `{tune}` to use these techniques.
3.  Continue applying the concepts and functions used from the previous
    sessions.

Specific "anti"-objectives:

-   Same as the "anti"-objectives of @sec-stats-analysis-basic.

## Exercise: Run model with a different metabolite

> Time: \~10 minutes.

Try running the same model workflow but run the model with
`metabolite_lipid_ch_3_1` instead of `metabolite_cholesterol`.

1.  In the `doc/lesson.Rmd` file, create a new `###` header and code
    chunk at the bottom of the R Markdown file (see the scaffold
    template below). You can use the code below as a scaffold, you can
    even copy and paste it.
2.  Copy and paste the previous workflow we created above and then
    replace all `metabolite_cholesterol` with `metabolite_lipid_ch_3_1`
    (or copy and paste the scaffold below).
3.  Run the code to get the results for `metabolite_lipid_ch_3_1`. What
    is the coefficient (`estimate`) for the `metabolite_lipid_ch_3_1`
    compared to the `metabolite_cholesterol` result?
4.  Use `r run_styler_text` on the R Markdown file. Then commit the
    changes to the Git history.
5.  Notice the only difference between the code running the models on
    `metabolite_cholesterol` compared to models on
    `metabolite_lipid_ch_3_1`? Keep this in mind for the next exercise
    :wink:

Use the scaffolding below as a guide. *Note*: The code has been
purposefully written poorly so you can use `{styler}` to fix it up.

    ### Model with a different metabolite
    ``` {{r}}
    create_model_workflow(___()%>%set_engine("glm"),
    lipidomics_wide%>%create_recipe_spec(___)
    ) %>%fit( lipidomics_wide ) %>% 
    tidy_model_output()
    ```

```{r solution-model-with-different-metabolite}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
create_model_workflow(
  logistic_reg() %>%
    set_engine("glm"),
  lipidomics_wide %>%
    create_recipe_spec(metabolite_lipid_ch_3_1)
) %>%
  fit(lipidomics_wide) %>%
  tidy_model_output()
```

## Exercise: How would we use functional programming to run multiple models?

> Time: \~20 minutes.

Functional programming underlies many core features of running
statistical methods on data. This exercise is meant for you to review
this concept and try to think of it in the context of statistcal
modeling.

-   For 10 minutes, go to the sections on [Function
    Programming](https://r-cubed-intermediate.rostools.org/dry-functionals.html#functional-programming)
    in the Intermediate R course as well as the
    [split-apply-combine](https://r-cubed-intermediate.rostools.org/dry-functionals.html#split-apply-combine-technique-and-functionals)
    and review the concepts.

-   For 8 minutes, discuss with your neighbour how we can use functional
    programming to apply the model to each metabolite. Try to think how
    the code would look like to do that. You don't need to write real R
    code, but if writing pseudocode helps, go right ahead! Also, don't
    look ahead :wink:

-   For the remaining time, we will discuss as the whole group some of
    the things people thought of.

::: {.callout-note appearance="minimal" collapse="true"}
## Instructor note

After they've finished, either write pseudocode in RStudio or draw this
out on a whiteboard if it is available. There will probably be several
different approaches, many of which could also be implemented just fine.
Ultimately we will replace `create_recipe_spec(metabolite_...)` with
`create_recipe_spec(starts_with("metabolite_"))`.
:::

## Apply logistic regression to each metabolite

You might have thought of some ideas to run the model on each metabolite
based on the `lipidomics_wide` dataset. However, these types of
"split-apply-combine" tasks are best done using data in the long form.
So we'll start with the original `lipidomics` dataset. The first thing
we want to do is convert the metabolite names into snake case:

```{r chain-col-to-snakecase}
lipidomics %>%
  column_values_to_snakecase(metabolite)
```

The next step is to split the data up. We could use `group_by()`, but in
order to make the most use of `{purrr}` functions like `map()`, we will
use `group_split()` to convert the data frame into a set of
lists[^stats-analyses-multiple-1]. Let's first add `{purrr}` as a
dependency:

[^stats-analyses-multiple-1]: There is probably a more efficient way of
    coding this instead of making a list, but as the saying goes
    ["premature optimization is the root of all
    evil"](https://stackify.com/premature-optimization-evil/). For our
    purposes, this is a very good approach, but for very large datasets
    and hundreds of potential models to run, this method would need to
    be optimized some more.

```{r purrr-to-deps}
#| purl: true
#| eval: false
use_package("purrr")
```

```{r chain-split-by-metabolite}
lipidomics %>%
  column_values_to_snakecase(metabolite) %>%
  group_split(metabolite)
```

Since logistic regression needs one row to be a single person, we'll use
`map()` to apply our custom function `metabolites_to_wider()` on each of
the split list items:

```{r chain-map-to-wider}
#| eval: false
lipidomics %>%
  column_values_to_snakecase(metabolite) %>%
  group_split(metabolite) %>%
  map(metabolites_to_wider)
```

```{r exec-only-chain-map-to-wider}
#| eval: true
#| echo: false
.Last.value %>%
  map(metabolites_to_wider)
```

Alright, we now have one data frame with only the single metabolite per
list item. While again this bit of code is only a few lines, it also
represents the conceptual action of "splitting the data into a list by
metabolites". So let's create a function for this. Add the
`r run_roxygen_comments`, run `r run_styler_text`, move into the
`R/functions.R` file, and then `source()` the file.

```{r new-function-split-by-metabolite}
#' Convert the long form dataset into a list of wide form data frames.
#'
#' @param data The lipidomics dataset.
#'
#' @return A list of data frames.
#'
split_by_metabolite <- function(data) {
  data %>%
    column_values_to_snakecase(metabolite) %>%
    dplyr::group_split(metabolite) %>%
    purrr::map(metabolites_to_wider)
}
```

The code should look like:

```{r split-by-metabolite}
lipidomics %>%
  split_by_metabolite()
```

Like we did with the `metabolite_to_wider()`, we need to pipe the output
into another `map()` function that has a custom function that runs the
models. We don't have this function yet, so we need to create it.
Convert the code we've used previously into a function, replacing
`lipidomics` with `data` and using `starts_with("metabolite_")` within
the `create_recipe_spec()`. Add the `r run_roxygen_comments`, run
`r run_styler_text`, move into the `R/functions.R` file, and then
`source()` the file.

```{r new-function-generate-model-results}
#' Generate the results of a model
#'
#' @param data The lipidomics dataset.
#'
#' @return A data frame.
#'
generate_model_results <- function(data) {
  create_model_workflow(
    parsnip::logistic_reg() %>%
      parsnip::set_engine("glm"),
    data %>%
      create_recipe_spec(tidyselect::starts_with("metabolite_"))
  ) %>%
    parsnip::fit(data) %>%
    tidy_model_output()
}
```

Then we add it to the end of the pipe, but using `map_dfr()` to convert
to a data frame:

```{r chain-generate-model-results}
lipidomics %>%
  split_by_metabolite() %>%
  map_dfr(generate_model_results)
```

Since we are only interested in the results for the model results for
the metabolites, let's keep only `terms` that are metabolites using
`filter()` and `str_detect()`.

```{r chain-filter-terms}
model_estimates <- lipidomics %>%
  split_by_metabolite() %>%
  map_dfr(generate_model_results) %>%
  filter(str_detect(term, "metabolite_"))
model_estimates
```

Wow! We're basically at our first `{targets}` output! Before continuing,
there is one aesthetic thing we can add: The original variable names,
rather than the snake case version. Since the original variable still
exists in our `lipidomics` dataset, we can join it to the
`model_estimates` object with `full_join()`, along with a few other
minor changes. First, we'll `seelct()` only the `metabolite` and then
create a duplicate column of `metabolite` called `term` (to match the
`model_estimates`) using `mutate()`.

```{r duplicate-original-vars}
lipidomics %>%
  select(metabolite) %>%
  mutate(term = metabolite)
```

Right after that we will use our custom `column_values_to_snakecase()`
function on the `term` column.

```{r dup-column-to-snakecase}
lipidomics %>%
  select(metabolite) %>%
  mutate(term = metabolite) %>%
  column_values_to_snakecase(term)
```

We can see that we are missing the `metabolite_` text before each snake
case'd name, so we can add that with `mutate()` and `str_c()`:

```{r dup-column-append-metabolite}
lipidomics %>%
  select(metabolite) %>%
  mutate(term = metabolite) %>%
  column_values_to_snakecase(term) %>%
  mutate(term = str_c("metabolite_", term))
```

There are 504 rows, which we only need the unique values of `term` and
`metabolite` using `distinct()`.

```{r dup-column-distinct}
lipidomics %>%
  mutate(term = metabolite) %>%
  column_values_to_snakecase(term) %>%
  mutate(term = str_c("metabolite_", term)) %>%
  distinct(term, metabolite)
```

The last step is to `right_join()` with the `model_estimates`:

```{r dup-column-full-join}
lipidomics %>%
  mutate(term = metabolite) %>%
  column_values_to_snakecase(term) %>%
  mutate(term = str_c("metabolite_", term)) %>%
  distinct(term, metabolite) %>%
  right_join(model_estimates, by = "term")
```

Awesome :smile: Now can you guess what we are going to do next? That's
right, making a function of both the model creation code and this code
to add the original variable names. Then we can add our first
`{targets}` output!

## Exercise: Creating functions for model results and adding as a target in the pipeline

> Time: \~25 minutes.

Convert the code that calculates the model estimates as well as the code
that adds the original metabolite names into functions. Start with the
code for the metabolite names, using the scaffold below as a starting
point.

1.  Name the new function `add_original_metabolite_names`.
2.  Within the `function()`, add two arguments, where the first is
    called `model_results` and the second is called `data`.
3.  Paste the code we created into the function, replacing `lipidomics`
    with `data` and `model_estimates` with `model_results`.
4.  Add `dplyr::` and `stringr::` before their respective functions.
5.  Add the `r run_roxygen_comments`.
6.  Run `r run_styler_text` to the file to fix up the code.
7.  Cut and paste the function over into the `R/functions.R` file.
8.  Commit the changes you've made so far.

``` r
___ <- function(___, ___) {
  ___ %>%
}
```

```{r solution-new-function-add-original-metabolite-names}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
#' Add the original metabolite names (not as snakecase) to the model results.
#'
#' @param model_results The data frame with the model results.
#' @param data The original lipidomics dataset.
#'
#' @return A data frame.
#'
add_original_metabolite_names <- function(model_results, data) {
  data %>%
    dplyr::mutate(term = metabolite) %>%
    column_values_to_snakecase(term) %>%
    dplyr::mutate(term = stringr::str_c("metabolite_", term)) %>%
    dplyr::distinct(term, metabolite) %>%
    dplyr::right_join(model_results, by = "term")
}
```

Do the same thing with the code that creates the model results, using
the scaffold below as a starting point. The only difference here is that
we would like to save the model results to a CSV file in the `data/`
folder so we can more easily share the results.

``` r
calculate_estimates <- function(data, csv_path = "data/model-estimates.R") {
  model_estimates <- ___ %>%
    # All the other code to create the results
    ___ %>% 
    add_original_metabolite_names(data) 
  readr::write_csv(model_estimates, here::here(csv_path))
  output_switch_for_targets(model_estimates, csv_path)
}
```

```{r solution-new-function-calculate-estimates}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
#' Calculate the estimates for the model for each metabolite.
#'
#' @param data The lipidomics dataset.
#'
#' @return A data frame.
#'
calculate_estimates <- function(data, csv_path = "data/model-estimates.csv") {
  model_estimates <- data %>%
    column_values_to_snakecase(metabolite) %>%
    dplyr::group_split(metabolite) %>%
    purrr::map(metabolites_to_wider) %>%
    purrr::map_dfr(generate_model_results) %>%
    dplyr::filter(stringr::str_detect(term, "metabolite_")) %>%
    add_original_metabolite_names(data)
  readr::write_csv(model_estimates, here::here(csv_path))
  output_switch_for_targets(model_estimates, csv_path)
}
```

Lastly, add the model results output to end of the `_targets.R` file,
using the below scaffold as a guide.

1.  Use `df_model_estimates` for the `name`.
2.  Use the `calculate_estimates()` function in `command`, with
    `lipidomics` and `"data/model-estimates.csv"` as the arguments.
3.  Run `r run_styler_text` and than run `r run_tar_vis_text` to see if
    the new target gets detected. If it does, than run
    `r run_tar_make_text`.
4.  Commit the changes to the Git history.

``` r
list(
  ...,
  list(
    name = ___,
    command = ___(___, ___),
    format = "file"
  )
)
```

```{r purl-only-model-estimates-to-targets}
#| eval: false
#| echo: false
#| purl: true
update_target_calc_est <- '
  ),
  list(
    name = df_model_estimates,
    command = calculate_estimates(lipidomics, "data/model-estimates.csv"),
    format = "file"
  )
)
'
# print_lines("_targets.R")
revise_by_line_num(
  path = "_targets.R",
  insert_text = update_target_calc_est,
  # TODO: Modify these numbers.
  remove_original_lines = -30,
  insert_at_line = 31
)
targets::tar_visnetwork()
targets::tar_make()
git_ci("_targets.R", "Update targets with model results")
```

## Visualizing the model estimates

We've got one target done for the modeling stage, three more to go!
There are multiple ways of visualizing the results from models. A common
approach is to use a "dot-and-whisker" plot like you might see in a
meta-analysis. Often the "whisker" part is the measure of uncertainty
like the confidence interval, and the "dot" is the estimate. For the
confidence interval, we haven't calculated them at this point because
the typical approach doesn't exactly work for our data (tested before
the course). The next section will be covering another way of
determining uncertainty. For this plot though, we will use the standard
error of the estimate. Since we'll be using `{ggplot2}` to make figures,
we need to add it to the dependency list:

```{r ggplot2-to-deps}
#| purl: true
use_package("ggplot2")
```

Inside the `doc/lesson.Rmd`, let's create a new header and code chunk at
the bottom. We'll want to use `targets::tar_read(df_model_estimates)` so
that `{targets}` is aware that the R Markdown file is dependent on this
target. This `targets::tar_read()` will output a path to the CSV file,
so we will need to use `read_csv()` to load it in.

    ### Figure of model estimates

    ```{{r}}
    model_estimates <- targets::tar_read(df_model_estimates) %>% 
      read_csv()
    ```

```{r exec-only-model-estimates}
#| include: false
model_estimates <- lipidomics %>%
  calculate_estimates()
```

Then we'll start using `{ggplot2}` to visualize the results. For
dot-whisker plots, the "geom" we would use is called
`geom_pointrange()`. It requires four values:

-   `x`: This will be the "dot", should we will use the `estimate`
    column.
-   `y`: This is the categorical variable that the "dot" is associated
    with, in this case, it is the `metabolite` column.
-   `xmin`: This is the lower end of the "whisker". Since the
    `std.error` is a value representing uncertainty of the estimate on
    either side of it (`+` or `-`), we will need to substract
    `std.error` from the `estimate`.
-   `xmax`: This is the upper end of the "whisker". Like `xmin` above,
    but adding `std.error` instead.

```{r plot-estimates-pointrangne-only}
plot_estimates <- model_estimates %>%
  ggplot(aes(
    x = estimate, y = metabolite,
    xmin = estimate - std.error,
    xmax = estimate + std.error
  )) +
  geom_pointrange()
plot_estimates
```

Wow, there is definitely something wrong here. The values of the
estimate should realistically be somewhere between 0 (can't have a
negative odds) and 2 (in biology and health research, odds ratios are
rarely above 2), very unlikely more than 5. We will eventually need to
troubleshoot this issue, but for now, let's restrict the x axis to be
between 0 and 5.

```{r plot-estimates-coord-fixed}
plot_estimates +
  coord_fixed(xlim = c(0, 5))
```

There are so many things we could start investigating based on these
results in order to fix them up. But for now, this will do.

## Exercise: Add plot function as a target in the pipeline

> Time: \~15 minutes.

Hopefully you've gotten comfortable with the function-oriented workflow,
because we'll need to convert this code into a function and add it as a
target in the pipeline. Use the scaffold below as a guide.

1.  Replace `model_estimates` with `results`.
2.  Use the `output_switch_for_targets()` function we created to output
    either the `plot_results` or the `image_path`.
3.  Move the function into the `R/functions.R` file, add
    `r run_roxygen_comments`, and run `r run_styler_text`.

``` r
plot_estimates <- function(results, image_path) {
  plot_results <- ___ %>% 
    # Plot code here:
    ___
  ggplot2::ggsave(
   plot = plot_results,
   filename = here::here(image_path)
  )
  output_switch_for_targets(___, ___)
}
```

```{r solution-new-function-plot-estimates}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
#' Plot the estimates and standard errors of the model results.
#'
#' @param results The model estimate results.
#' @param image_path Path to where image will be saved.
#'
#' @return A ggplot2 figure.
#'
plot_estimates <- function(results, image_path) {
  plot_results <- results %>%
    ggplot2::ggplot(ggplot2::aes(
      x = estimate, y = metabolite,
      xmin = estimate - std.error,
      xmax = estimate + std.error
    )) +
    ggplot2::geom_pointrange() +
    ggplot2::coord_fixed(xlim = c(0, 5))

  ggplot2::ggsave(
    plot = plot_results,
    filename = here::here(image_path)
  )
  output_switch_for_targets(plot_results, image_path)
}
```

Then, after doing that, add the new function as a target in the
pipeline, name the new `name` as `fig_model_estimates`.

``` r
list(
  ...,
  tar_target(
    name = ___,
    command = plot_estimates(___, ___),
    format = "file"
  )
)
```

And add the `targets::tar_read()` in the R Markdown using
`knitr::include_graphics()`:

    ```{{r}}
    knitr::include_graphics(targets::tar_read(fig_model_estimates))
    ```

Run `r run_tar_make_text` to update the pipeline. Then commit the
changes to the Git history.

```{r purl-only-tar-make-fig-estimates}
#| eval: false
#| echo: false
#| purl: true
update_target_plot_est <- '),
tar_target(
  name = fig_model_estimates,
  command = plot_estimates(df_model_estimates, "images/fig-model-estimates"),
  format = "file"
)
)
'
# print_lines("_targets.R")
styler::style_file("_targets.R")
revise_by_line_num(
  path = "_targets.R",
  insert_text = update_target_calc_est,
  # TODO: Modify these numbers.
  remove_original_lines = -30,
  insert_at_line = 31
)

targets::tar_visnetwork()
targets::tar_make()
git_ci("_targets", "Add plot estimates target")
```

## Determine variability in model estimates with resampling

We've already noticed that there is something strange with the estimates
for some of the metabolites. What we want to do now is calculate a more
realistic, data-specific measure of variation. The way we do that is
through "resampling".

There are many resampling techniques, and they all have slightly
different uses. The one we will use is called the
["bootstrap"](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))[^stats-analyses-multiple-2].
What bootstrapping does is take the data and randomly resamples it as
many times as there are rows. This means you can potentially resample
*the same data (in our case, person) more than once* (called "with
replacement"). So by pure chance, you could theoretically have a
"resampled set" from `lipidomics` where all 36 rows are only duplicate
data on one person!

[^stats-analyses-multiple-2]: Another common technique is called
    ["v-fold
    cross-validation"](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation),
    which is provides a way of assessing how well the model as a whole
    performs at fitting the data, rather than where bootstrap determines
    how varied the estimate can be.

<!-- TODO: Make a diagram of this at some point. -->

What's the advantage of this? It is a way of directly calculating the
standard error from the data itself (rather than from a formula), so it
gives a more accurate view of how uncertain the model estimate for our
data. Usually, creating between 50 to 100 "resampled sets" is sufficient
to calculate a value for the variation. Because running models with
bootstrapped sets can take a long time to process, we will only do 10,
or fewer if your computer is slow.

The `{rsamples}` package handles "resampling". So let's add it to our
dependency list:

```{r rsamples-to-deps}
#| purl: true
#| eval: false
use_package("rsamples")
```

We will eventually run bootstraps on all the metabolites, so we will
need to use our `split_by_metabolite()` function first. But we will only
use the first item in that list (accessed with `[[1]]`). Create another
code chunk at the bottom of `doc/lesson.Rmd` to add this code:

```{r split-metabolite-for-bootstrap}
lipidomics_list <- lipidomics %>%
  split_by_metabolite()
```

::: callout-info
## Reading task: \~10 minutes

> *You don't need to run the code in this reading section*.

The `bootstraps()` function is how we create resampled sets, so let's
test it out. Since this is done randomly, we should use `set.seed()` in
order for the analysis to be reproducible. Nothing is truly random in
computers, but rather "pseudorandom". In order for our analysis to be
reproducible, we use `set.seed()` to force a specific "pseudorandom"
value.

```{r show-bootstrap-first-item}
set.seed(1324)
bootstraps(lipidomics_list[[1]], times = 10)
```

This output is called a "nested tibble". A nested tibble is a
tibble/data frame where one or more of the columns are actually a list
object. In our case, each bootstrapped set (marked by the `id`) has
instructions on how the resampled data will look like. We can see what
it looks like by accessing the `splits` column and looking at the first
item with `[[]]`:

```{r show-split-contents}
bootstraps(lipidomics_list[[1]], times = 10)$splits[[1]]
```

The contents of this resampled set are split into "analysis" sets and
"assessment" sets. You don't need to worry about what these mean or how
to use them, since a function we will later use handles it for us. But
to give you an idea of what bootstrapping is doing here, we can access
one of the sets with either the `analysis()` or `assessment()`
functions. We'll `arrange()` by `code` to show how we can have duplicate
persons when resampling:

```{r show-split-contents-analysis}
bootstraps(lipidomics_list[[1]], times = 10)$splits[[1]] %>%
  analysis() %>%
  arrange(code)
```

See how some `code` IDs are the same? Those are the same person that has
been selected randomly into this resampled set.
:::

Like we did with the previous modeling, we need to create a workflow
object. We'll use the first metabolite (`lipidomics_list[[1]]`) for now,
but will revise the code to eventually run the bootstrapping on all
metabolites.

```{r create-workflow-for-bootstrap}
workflow_for_bootstrap <- create_model_workflow(
  logistic_reg() %>%
    set_engine("glm"),
  lipidomics_list[[1]] %>%
    create_recipe_spec(starts_with("metabolite_"))
)
```

Previously, we used `fit()` on the workflow and on the data. Instead, we
will use `fit_resamples()` to run the model on the bootstrapped data.
Instead of the `data` argument in `fit()`, it is the `resamples`
argument where we provide the `bootstraps()` sets. We could run the code
with only the workflow object and the resampled data, but there's an
extra argument in `fit_resamples()` that `control`'s some actions taken
during the fitting by using the `control_resamples()` function. For
instance, we can save the predictions with `save_pred = TRUE` and we can
process the output with our `tidy_model_output()` function in the
`extract` argument. So let's do that. First, both `fit_resamples()` and
`control_resamples()` come from the `{tune}` package, so let's add it to
the dependencies first.

```{r tune-to-deps}
#| purl: true
#| eval: false
use_package("tune")
```

Now, we can write the code for `fit_resamples()` on the `bootstraps()`
of the first item in the `lipidomics_list` and setting the `control`
options with `control_resamples()`.

```{r fit-on-resampled-sets}
bootstrapped_results <- fit_resamples(
  workflow_for_bootstrap,
  resamples = bootstraps(lipidomics_list[[1]], times = 10),
  control = control_resamples(
    extract = tidy_model_output,
    save_pred = TRUE
  )
)
bootstrapped_results
```

You'll see that it gives another nested tibble, but with more columsn
included. Before we start selecting the results that we want, let's
convert the code above into a function, using the function-oriented
workflow we've used throughout the course.

```{r new-function-generate-model-variation}
#' Generate the model variation results using bootstrap on a single metabolite.
#'
#' @param data The lipidomics data.
#'
#' @return A nested tibble.
#'
generate_model_variation <- function(data) {
  create_model_workflow(
    parsnip::logistic_reg() %>%
      parsnip::set_engine("glm"),
    data %>%
      create_recipe_spec(tidyselect::starts_with("metabolite_"))
  ) %>% 
    tune::fit_resamples(
    resamples = rsample::bootstraps(data, times = 10),
    control = tune::control_resamples(
      extract = tidy_model_output,
      save_pred = TRUE
    )
  )
}
```

Re-writing the code to use the function, it becomes:

```{r}
bootstrapped_results <- lipidomics_list[[1]] %>% 
  generate_model_variation()
bootstrapped_results
```

```{r}
bootstrapped_results %>% 
  select(id, .extracts) %>%
  unnest(cols = .extracts)
```

```{r}
bootstrapped_results %>% 
  select(id, .extracts) %>%
  # Need to unnest twice since first `.extracts` is a nest of another two
  # columns of `.extracts` and `.config`.
  unnest(cols = .extracts) %>%
  unnest(cols = .extracts) %>%
  filter(str_detect(term, "metabolite_")) %>%
  add_original_metabolite_names(lipidomics) 
```

```{r}
tidy_bootstrap_output <- function(bootstrap_results) {
  bootstrap_results %>% 
    dplyr::select(id, .extracts) %>%
    # Need to unnest twice since first `.extracts` is a nest of another two
    # columns of `.extracts` and `.config`.
    tidyr::unnest(cols = .extracts) %>%
    tidyr::unnest(cols = .extracts) %>%
    dplyr::filter(stringr::str_detect(term, "metabolite_")) %>%
    add_original_metabolite_names(lipidomics) 
}
```

```{r}
lipidomics %>% 
  # calculate_variation()
  split_by_metabolite() %>% 
  .[1:2] %>% 
  map(generate_model_variation) %>% 
  map_dfr(tidy_bootstrap_output)
```

```{r}
calculate_variation <- function(data, results_path = "data/model-variation.csv") {
  bootstrapped_results <- data %>% 
    split_by_metabolite() %>% 
    map(generate_model_variation) %>% 
    map_dfr(tidy_bootstrap_output)
  readr::write_csv(bootstrapped_results, here::here(results_path))
  output_switch_for_targets(bootstrapped_results, results_path)
}

  

```

```{r plot-variation}
bootstrapped_results %>%
  select(id, .extracts) %>%
  # Need to unnest twice since first `.extracts` is a nest of another two
  # columns of `.extracts` and `.config`.
  unnest(cols = .extracts) %>%
  unnest(cols = .extracts) %>%
  filter(str_detect(term, "metabolite_")) %>%
  add_original_metabolite_names(lipidomics) %>%
  ggplot(aes(x = estimate)) +
  geom_dotplot() +
  facet_wrap(vars(metabolite))
```

## Visualizing the variability of results

-   Combine augment with rsamples and ggplot2 to plot different
    estimates

```{r}
#| eval: false
```

<!-- TODO: makke function for plot -->

## Exercise: Running multiple models by metabolite *and* re-sampled bootstrapped set

Using

## Exercise: Add functions into targets and R Markdown file

```{r}
# TODO: Reuse all chunks that have functions and place them here to purl
```

## Summary
