---
execute: 
  eval: false
---

# Creating automatic analysis pipelines {#sec-pipelines}

```{r setup}
#| include: false
#| eval: true
source(here::here("R/functions.R"))
library(tidyverse)
load(here::here("data/lipidomics.rda"))
```

{{< include ../includes/_wip.qmd >}}

::: {.callout-note appearance="minimal" collapse="true"}
## Instructor note

Before beginning, get them to recall what they remember of the previous
session, either with something like Mentimeter or verbally. Preferably
something like Mentimeter because it allows everyone to participate, not
just the ones who are more comfortable being vocal to the whole group.
:::

## Learning objectives

The overall objective for this session is to:

1.  Identify and apply an approach to creating an analysis pipeline that
    makes your analysis steps, from raw data to finished manuscript,
    explicitly defined so that updating it by either you or
    collaborators is as easy as running a single function.

More specific objectives are to:

1.  Describe the computational meaning of pipeline and how pipelines are
    often done in research. Explain why a well-designed pipeline can
    streamline your collaboration, reduce time spent doing an analysis,
    make your analysis steps explicit and easier to work on, and
    ultimately contribute to more fully reproducible research.
2.  Explain the difference between a "function-oriented" workflow vs a
    "script-oriented" workflow and why the function-based approach has
    multiple advantages from a time- and effort-efficiency point of
    view.
3.  Use the functions within `{targets}` to apply the concepts of
    building pipelines in your analysis project.
4.  Continue applying the concepts and functions used from the previous
    session.

## Exercise: How do you re-run analyses when something changes?

> Time: \~14 minutes.

We've all been in situations where something in our analysis needs to
change. Maybe we forgot to remove a certain condition (like unrealistic
BMI). Or maybe our supervisor suggests something we hadn't considered in
the analysis. Or maybe during peer review of our manuscript, a reviewer
makes a suggestion that would improve the understanding of the paper.
What ever the situation, we inevitably need to re-run our analysis. And
depending on what the change was, we might need to run the full analysis
all over again. So what is your **exact** workflow when you need to
re-run code and update your results? Assume it's a change somewhere
early in the data processing stage.

1.  Take 2 min to think about the workflow you use. Try to think of the
    *exact* steps you need to take, what *exactly* you do, and how long
    that usually takes.
2.  For 8 min, in your group share and discuss what you've thought. How
    do your experiences compare to each other?
3.  For the remaining time, we'll briefly share with everyone about what
    they've thought and discussed.

## What is a data analysis "pipeline"?

TODO: Should this be a reading exercise or the instructor talk about it?

A pipeline can be any process where the steps between a start and an end
point are very clear, explicit, and concrete. These highly distinct
steps can be manual, human involved or can be completely automated by a
robot or computer. For instance, in car factories, the pipeline from the
input raw materials to the output vehicle are extremely well described
and implemented. Or like during the pandemic, the pipeline for testing
(at least in Denmark and several other countries) was highly structured
and clear for the workers doing the testing and the people having the
test done: A person goes in, scans their ID card, has the test done,
worker inputs the results, results get sent immediately to the health
agency as well as to the person based on their ID contact information
(or via a secure app).

However, in research, especially around data collection and analysis, we
often hear or read about "pipelines". But looking closer, these aren't
actual pipelines because the individual steps are not very clear and not
well described, often requiring a fair amount of manual human attention
and intervention. Particularly within computational environments, a
pipeline is when there is minimal to *no* human intervention from raw
input to finished output. Why aren't these data "pipelines" in research
actual pipelines? Because:

1.  Anything with data ultimately must be on the computer,
2.  Anything automatically done on the computer must be done with code,
3.  Not all researchers write code,
4.  Researchers who do write code rarely publish and share it,
5.  Code that is shared or published (either publicly or within the
    research group) is not written in a way that allows a pipeline to
    exist,
6.  And, research is largely non-reproducible [@Trisovic2022;
    @Seibold2021; @Laurinavichyute2021].

A data analysis pipeline would by definition be a reproducible,
readable, and code-based data analysis. we researchers as a group have a
long way to go before we can start realistically implementing data
analysis pipelines.

This isn't to diminish the work of researchers, but a basic observation
on the systemic, social, and structural environment that we researchers
work in. We as researchers are not trained on how to write code, nor do
we have a strong culture and incentive structure around learning,
sharing, reviewing, and improving code. Nor are we often allowed to get
(or use) funds to hire people who *are* trained on and skilled in
programmatic thinking and programming. Otherwise courses like this
wouldn't need to exist :shrug:

So how would a data analysis pipeline look like? Before we get to that
though, we have to separate two things: exploratory data analysis and
final paper data analysis. In exploratory data analysis, there will
likely be a lot of manual, interactive steps involved that may or may
not need to be explicitly stated and included in the analysis plan and
pipeline. But for the final paper and what results would be included, we
generally have some basic first ideas. Let's list a few items that we
would want to do before the primary statistical analysis:

1.  A table of some basic descriptive statistics of the study
    population, such as mean, standard deviation, or counts of basic
    discrete data (like treatment group).
2.  A figure visualizing the counts or other statistics for variables of
    interest that are discrete/categorical. For the data in this course,
    that would be variables like gender and age.
3.  A figure showing the distribution of your main variables of
    interest. In this case, ours are the lipidomic variables.
4.  The paper with the results included.

```{mermaid fig-pipeline-schematic}
%%| label: fig-pipeline-schematic
%%| fig-cap: Very simple flow chart of steps in an analysis pipeline.
%%| echo: false
%%| eval: true
%%{init:{'theme':'forest', 'flowchart':{'nodeSpacing': 20, 'rankSpacing':30}, 'themeVariables': { 'edgeLabelBackground': 'transparent'}}}%%
graph LR
    data[Data] --> fn_desc{{function}} 
    fn_desc --> tab_desc[Table:<br>descriptive<br>statistics]
    data --> fn_plot{{function}}
    fn_plot{{function}} --> plot_discr[Plot:<br>Discrete<br>variables]
    tab_desc --> paper[Paper]
    plot_discr --> paper
    data --> fn_plot_vars{{function}}
    fn_plot_vars{{function}} --> plot_distrib[Plot:<br>Continuous<br>variables]
    plot_distrib --> paper

linkStyle 0,1,2,3,4,5,6,7,8 stroke-width:1px;
```

Now that we conceptually have drawn out the sets of tasks to complete in
our pipeline, we can start using R to build it.

## Using targets to manage the pipeline

There are a few packages to help build pipelines in R, but the most
commonly used, well-designed, and maintained one is called `{targets}`.
With this package, you specify outputs you want to create and
`{targets}` will track them for you. So it will know which output
depends on which other one and which ones need to be updated.

::: {.callout-note appearance="minimal" collapse="true"}
## Instructor note

Ask participants, which do they think it is: a build dependency or a
workflow dependency. Because it is directly used to run analyses and
process the data, it would be a build dependency.
:::

First, since we are using `{renv}`, we need to install `{targets}` in
the project environment. And since `{targets}` is a build dependency, we
add it to the `DESCRIPTION` file with:

```{r add-targets-as-dep}
#| purl: true
use_package("targets")
```

After it finishes being added to the project R library, let's set up our
project to start using it!

```{r use-targets}
#| purl: true
targets::use_targets()
```

This will add several files:

```{r purl-only-use-targets-files}
#| eval: false
#| echo: false
#| purl: true
# TODO: Use to list out files created to paste into book.
fs::dir_ls(regexp = "run.*|_targets.*")
git_ci(c("run.R", "run.sh", "_targets.R"),
       "Start using targets")
```

The most important file is the `_targets.R` file. The other two files
are used for other situations (like running on a Linux server) that we
won't cover in the course. Before we continue though, let's **commit
these new files to the Git history**.

::: {.callout-note appearance="minimal" collapse="true"}
## Instructor note

Let them read it before going over it again to reinforce
function-oriented workflows and how `{targets}` and the `tar_target()`
works.
:::

::: callout-info
## Reading task: \~8

Next, open up the `_targets.R` and we'll take a look at what it
contains.

::: {.callout-info collapse="true"}
## Click this to see the file contents

```{r show-use-targets-file-contents}
#| eval: true
#| echo: false
#| code-fold: true
fs::path_package("targets", "pipelines", "use_targets.R") |>
  readLines() |>
  cat(sep = "\n")
```
:::

One small thing to notice are the `#nolint` comments used throughout.
These "tags" are used to tell `{lintr}` to ignore these lines when
running the lintr.

Next, notice `tar_target()` function used at the end of the script.
There are two main arguments for it `name` and `command`. The way that
`{targets}` works is similar to how you'd write R code to assign the
output of a function to an object.

``` r
object_name <- function_in_command(input_arguments)
```

Is the same as:

``` r
tar_target(
  name = object_name,
  command = function_in_command(input_arguments)
)
```

What this means is that `{targets}` follows a ["function-oriented"
workflow](https://books.ropensci.org/targets/functions.html#problems-with-script-based-workflows),
not a "script-based" workflow. What's the difference? In a
script-oriented workflow, each R file is run in a specific order, so you
might end up with an R file that has code like:

``` r
source("R/1-process-data.R")
source("R/2-basic-statistics.R")
source("R/3-create-plots.R")
source("R/4-linear-regression.R")
```

While in a function-oriented workflow, it might look more like:

``` r
source("R/functions.R")
raw_data <- load_raw_data("file/path/data.csv")
processed_data <- process_data(raw_data)
basic_stats <- calculate_basic_statistics(processed_data)
simple_plot <- create_plot(processed_data)
model_results <- run_linear_reg(processed_data)
```

With this workflow, each function takes an input dataset and contains
all the code to create the results into one output, like a figure in a
paper. If you've taken the [intermediate R
course](https://r-cubed-intermediate.rostools.org/), you'll notice that
this function-oriented workflow is the workflow we covered. There are so
many advantages to this type of workflow and is the reason many powerful
R packages are designed around making use of this type workflow.

If we take these same code and convert it into the `{targets}` format,
the end of `_targets.R` file would like this:

``` r
list(
  tar_target(
    name = raw_data,
    command = load_raw_data("file/path/data.csv")
  ),
  tar_target(
    name = processed_data,
    command = process_data(raw_data)
  ),
  tar_target(
    name = basic_stats,
    command = calculate_basic_statistics(processed_data)
  ),
  tar_target(
    name = simple_plot,
    command = create_plot(processed_data)
  ),
  tar_target(
    name = model_results,
    command = run_linear_reg(processed_data)
  ),
  tar_target(
    name = processed_data,
    command = process_data(raw_data)
  ),
)
```
:::

Let's start writing code to create the four items we listed above: some
descriptive statistics, a plot of some discrete data, a plot of the
continuous lipid variables, and a report (R Markdown). Since we'll use
`{tidyverse}`, specifically `{dplyr}`, to calculate the summary
statistics, we need to add it to our dependencies and install it in the
project library. `{tidyverse}` is a special "meta"-package so we need to
add it to the `"depends"` section of the `DESCRIPTION` file.

```{r add-tidyverse-deps}
#| purl: true
use_package("tidyverse", "depends")
use_package("dplyr")
```

Open up the `doc/lesson.Rmd` file and create a new header and code chunk
at the bottom of the file.

    ## Basic statistics

    ```{{r setup}}
    library(tidyverse)
    load(here::here("data/lipidomics.rda"))
    ```

    ```{{r basic-stats}}

    ```

```{r purl-only-markdown-basic-stats}
#| eval: false
#| echo: false
#| purl: true
c("\n## Basic statistics\n\n```{r setup}\nlibrary(tidyverse)", 
"load(here::here('data/lipidomics.rda'))\n```\n\n") |>
  paste0(collapse = "\n") |> 
  write_to_file("doc/lesson.Rmd")
git_ci("doc/lesson.Rmd", "Add section on basic stats to rmd")
```

TODO: Save this markdown text in order to add to project via purl?

We want to calculate the mean and SD for each metabolite and than, to
make it more readable, to round the numbers to one digit. We covered
this in the
[functionals](https://r-cubed-intermediate.rostools.org/dry-functionals.html#summarizing-long-data-like-the-rr-dataset)
session of the intermediate course, so we will apply these same
principles and code here. To do that, we need to use `group_by()` on the
`metabolites`, use `across()` inside `summarise()` so we can give it the
`mean()` and `sd()` functions, followed by `mutate()`ing each numeric
column (`across(where(is.numeric))`) to `round()` to 1 digit. Let's
write out the code!

```{r mean-sd-by-each-metabolite}
#| eval: true
lipidomics %>%
  group_by(metabolite) %>%
  summarise(across(value, list(mean = mean, sd = sd))) %>%
  mutate(across(where(is.numeric), round, digits = 1))
```

After that, run `r run_styler_text` on the file. Than we will **commit**
the changes to the Git history.

## Exercise: Convert summary statistics code into a function

> Time: \~25 minutes.

While inside the `doc/lesson.Rmd` file, use the "function-oriented"
workflow, as taught in the [intermediate
course](https://r-cubed-intermediate.rostools.org/dry-functions.html#the-basics-of-a-function),
to take the code we wrote above and convert it into a function. Complete
these tasks:

1.  Wrap the code with `function() {...}` and name the new function
    `descriptive_stats`.
2.  Replace `lipidomics` with `data` and put `data` as an argument
    inside the brackets of `function()`.
3.  Add `dplyr::` to the start of each `{dplyr}` function used inside
    your function.
4.  Run `r run_styler_text` and `r run_lintr_text` on the code to make
    sure it is formatted correctly. You might need to manually force a
    styling if lines are too long.
5.  With the *cursor* inside the function, add some roxygen
    documentation with `Ctrl-Shift-P` followed by typing "roxygen
    comment". Remove the lines that contain `@examples` and `@export`,
    then fill in the other details (like the `@params` and `Title`). In
    the `@return` section, write "A data.frame/tibble."
6.  Cut and paste the function over into the `R/functions.R` file.
7.  Source the `R/functions.R` file (`Ctrl-Shift-S`) and then test the
    code by running `descriptive_stats(lipidomics)` in the Console. If
    it works, do the last task.
8.  Save both files and then open the Git interface and commit the
    changes you made to them.

Here is some scaffolding to help you get started:

``` r
descriptive_stats <- function(___) {
  ___
}
```

```{r solution-summary-stats-to-function}
#| code-fold: true
#| code-summary: "**Click for a potential solution**. Only click if you are struggling or are out of time."
#' Calculate descriptive statistics of each metabolite.
#'
#' @param data Lipidomics dataset.
#'
#' @return A data.frame/tibble.
#'
descriptive_stats <- function(data) {
  data %>%
    dplyr::group_by(metabolite) %>%
    dplyr::summarise(dplyr::across(value, list(mean = mean, sd = sd))) %>%
    dplyr::mutate(dplyr::across(dplyr::where(is.numeric), round, digits = 1))
}
```

```{r purl-only-summary-stats-function}
#| purl: true
#| echo: false
"
#' Calculate descriptive statistics of each metabolite.
#'
#' @param data Lipidomics dataset.
#'
#' @return A data.frame/tibble.
#'
descriptive_stats <- function(data) {
  data %>%
    dplyr::group_by(metabolite) %>%
    dplyr::summarise(dplyr::across(value, list(mean = mean, sd = sd))) %>%
    dplyr::mutate(dplyr::across(dplyr::where(is.numeric), round, digits = 1))
" |>
  readr::write_lines(here::here("R/functions.R"))
git_ci("R/functions.R", "Add function to create descriptive stats.")
```

## Adding a step in the pipeline

```{r}
list(
  tar_target(
    name = lipidomics,
    command = load(here::here("data/lipidomics.rda"))
  ),
  tar_target(
    name = stats_by_metabolite,
    command = descriptive_stats(lipidomics)
  )
)
```

Or Palette "run targets"

```{r}
targets::tar_make()
```

Try to run, won't because targets doesn't know about the packaages
needed.

use tar_option_set() and packages argument

```{r}
desc::desc_get_deps()$package[-1]
```

```{r}
# Need to?
use_git_ignore("_targets")
```

Try running again, might be able to.

```{r}
targets::tar_visnetwork()
```

```{r}
targets::tar_outdated()
```

## Exercise: Examine the contents of the `_targets/` folder

> Time: \~5 minutes.

Like you did with `{renv}`, go into the `_targets/` folder and take a
look around.

TODO: Need to run on my end to know what to look for.

## Exercise: Update function to also calculate median and IQR

> Time: \~8 minutes.

Let's make a change to our function and test out how the
`tar_outdated()` and `tar_visnetwork()` work.

1.  Open up the `R/functions.R` file.
2.  Add median and interquartile range (IQR) to the `summarise()`
    function, by adding it to the end of `list(mean = mean, sd = sd)`,
    after the second `sd`. Note, IQR should look like `iqr = IQR` since
    we want the output columns to have a lowercase for the column names.
3.  Run `tar_outdated()` and `tar_visnetwork()` in the Console (or by
    using the Command Palette `Ctrl-Shift-P`, then "targets outdated" or
    "targets visualize"). What does it show?
4.  Run `r run_styler_text` and `r run_lintr_text`. You might need to
    force a reformat if the code is too long by highlighting the line
    and using `Ctrl-Shift-P`, then "reformat".
5.  Run `tar_make()` in the Console (or `Ctrl-Shift-P`, then "targets
    run", selecting the "background" option). Re-check for outdated
    targets and visualize the network again.
6.  Open up the Git interface and commit the changes to the Git history.

```{r solution-more-summary-stats-to-function}
#| code-fold: true
#| code-summary: "**Click for a potential solution**. Only click if you are struggling or are out of time."
#' Calculate descriptive statistics of each metabolite.
#'
#' @param data Lipidomics dataset.
#'
#' @return A data.frame/tibble.
#'
descriptive_stats <- function(data) {
  data %>%
    dplyr::group_by(metabolite) %>%
    dplyr::summarise(dplyr::across(value, list(
      mean = mean,
      sd = sd,
      median = median,
      iqr = IQR
    ))) %>%
    dplyr::mutate(dplyr::across(dplyr::where(is.numeric), round, digits = 1))
}
```

```{r purl-only-more-summary-stats-to-function}
#| eval: false
#| echo: false
#| purl: true
"
#' Calculate descriptive statistics of each metabolite.
#'
#' @param data Lipidomics dataset.
#'
#' @return A data.frame/tibble.
#'
descriptive_stats <- function(data) {
  data %>%
    dplyr::group_by(metabolite) %>%
    dplyr::summarise(dplyr::across(value, list(
      mean = mean,
      sd = sd,
      median = median,
      iqr = IQR
    ))) %>%
    dplyr::mutate(dplyr::across(dplyr::where(is.numeric), round, digits = 1))
}
" |>
  readr::write_lines(here::here("R/functions.R"), append = FALSE)
```

## Saving intermediate analysis outputs

Be aware that intermediate data gets saved to `_targets/` folder. So we
want Git to ignore it, so we don't accidentally save things we don't
want in the history. If you need intermediate output, save as CSV to
`data/`.

-   basic plot of data

    -   Very basic plot of count of gender, age, and class

```{r}
use_package("ggplot2")
```

```{r}
lipidomics %>%
  distinct(code, gender, class) %>%
  ggplot(aes(x = class, fill = gender)) +
  geom_bar(position = "dodge")
ggsave(here::here("images/plot-count-stats.png"))

ggplot(lipidomics, aes(x = value)) +
  geom_histogram() +
  facet_wrap(vars(metabolite), scales = "free")
ggsave(here::here())
```

For targets that create an intermediate or output file, targets needs
the file path as an output of the function when used within
`use_target()`. In order to make it easier to include this in later
functions, let's make a function for it.

```{r output-switch}
#' Switch output based on if it is interactive, otherwise it is a file path.
#'
#' @param output Output object to send to screen when running interactively.
#' @param path The file path to where the intermediate object is saved.
#'
#' @return An object or a file path.
#'
output_switch_for_targets <- function(output, path) {
  if (interactive()) {
    output
  } else {
    here::here(path)
  }
}
```

```{r purl-only-output-switch}
#| eval: false
#| echo: false
#| purl: true
"
#' Switch output based on if it is interactive, otherwise it is a file path.
#'
#' @param output Output object to send to screen when running interactively.
#' @param path The file path to where the intermediate object is saved.
#'
#' @return An object or a file path.
#'
output_switch_for_targets <- function(output, path) {
  if (interactive()) {
    output
  } else {
    here::here(path)
  }
}
" |>
  write_to_file("R/functions.R", append = TRUE)
git_ci("R/functions.R", "Add function to switch output for targets use.")
```

## Exercise: Convert the plot code to a function and add as a target

> Time: \~20 minutes.

For now, we will only take the code to make the bar plot and convert it
into a function. Just like you did with the `descriptive_stats()`
function in the exercise above, complete these tasks:

1.  Wrap the code with `function() {...}` and name the new function
    `plot_count_stats`.
2.  Replace `lipidomics` with `data` and put `data` as an argument
    inside the brackets of `function()`.
3.  Add `ggplot2::` and `dplyr::` to the start of each `{ggplot2}` and
    `{dplyr}` function used inside your function.
4.  At the end of the function right after the `ggplot2::ggsave()` code,
    use the `output_switch_for_targets()` function we create. See the
    scaffolding code below.
5.  Run `r run_styler_text` and `r run_lintr_text` on the code to make
    sure it is formatted correctly. You might need to manually force a
    styling if lines are too long.
6.  With the *cursor* inside the function, add some roxygen
    documentation with `Ctrl-Shift-P` followed by typing "roxygen
    comment". Remove the lines that contain `@examples` and `@export`,
    then fill in the other details (like the `@params` and `Title`). In
    the `@return` section, write "A plot object or a file path."
7.  Cut and paste the function over into the `R/functions.R` file.
8.  Source the `R/functions.R` file (`Ctrl-Shift-S`) and then test the
    code by running `plot_count_stats(lipidomics)` in the Console. If it
    works, do the last task.
9.  Save both files and then open the Git interface and commit the
    changes you made to them.

Use this scaffolding code to help guide you to write the code into a
function.

``` r
plot_count_stats <- function(___) {
  plot_output <- ___
  
  ggplot2::ggsave(
    plot = plot_output, 
    filename = here::here("images/plot-count-stats.png")
  )
  output_switch_for_targets(plot_output, ___)
}
```

```{r solution-plot-count-as-function}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
## This should be in the R/functions.R file.
#' Plot for basic count data.
#'
#' @param data The lipidomics dataset.
#'
#' @return A ggplot2 graph, or a file path.
#'
plot_count_stats <- function(data) {
  plot_output <- data %>%
    dplyr::distinct(code, gender, class) %>%
    ggplot2::ggplot(ggplot2::aes(x = class, fill = gender)) +
    ggplot2::geom_bar(position = "dodge")
  ggplot2::ggsave(
    plot = plot_output,
    filename = here::here("images/plot-count-stats.png")
  )
  output_switch_for_targets(plot_output, "images/plot-count-stats.png")
}
```

```{r purl-only-plot-count-as-function}
#| eval: false
#| echo: false
#| purl: true
"
#' Plot for basic count data.
#'
#' @param data The lipidomics dataset.
#'
#' @return A ggplot2 graph, or a file path.
#'
plot_count_stats <- function(data) {
  plot_output <- data %>%
    dplyr::distinct(code, gender, class) %>%
    ggplot2::ggplot(ggplot2::aes(x = class, fill = gender)) +
    ggplot2::geom_bar(position = 'dodge')
  ggplot2::ggsave(
    plot = plot_output,
    filename = here::here('images/plot-count-stats.png')
  )
  output_switch_for_targets(plot_output, 'images/plot-count-stats.png')
}
" |>
  write_to_file("R/functions.R")
git_ci("R/functions.R", "Add function to create plot of count stats.")
```

## Including intermediate output results in targets

Try to use file names consistently.

```{r}
use_target(
  name = plot_count_stats,
  command = plot_count_stats(lipidomics),
  format = "file"
)
```

## Incorporating targets with R MArkdown

-   basic report of data using summaries and plot

    -   Add Rmd file to targets
    -   Use `tar_render()`

```{r}
targets::tar_read() # only one output
targets::tar_load() # can load several at once
```

```{r}
list(
  tar_render()
)
```

```{r}
create_table <- function(data) {
  data %>%
    mutate(MeanSD = glue::glue("{value_mean} ({value_sd})")) %>%
    select(Metabolite = metabolite, `Mean SD` = MeanSD) %>%
    knitr::kable(caption = "Descriptive statistics of the metabolites.")
}
```

## Exercise: Removing the stored pipeline data

> Time: \~10 minutes.

Like with `{renv}`, sometimes you need to start from the beginning and
clean everything up because there's an issue that you can't seem to fix.
In this case, `{targets}` has a few functions to help out. Here are four
that you can use to delete stuff (also [described on the targets
book](https://books.ropensci.org/targets/data.html#cleaning-up-local-internal-data-files)):

`tar_invalidate()`

:   This removes the metadata on the target in the pipeline, but doesn't
    remove the object itself (which `tar_delete()` does). This will tell
    `{targets}` that the target is out of date, since it has been
    removed, even though the data object itself isn't present. You can
    use this like you would `select()`, by naming the objects directly
    or using the `{tidyselect}` helpers (e.g. `everything()`,
    `starts_with()`).

`tar_delete()`

:   This deletes the stored objects (e.g. the `lipidomics` or
    `stats_by_metabolite`) inside `_targets/`, but does not delete the
    record in the pipeline. So `{targets}` will see that the pipeline
    doesn't need to be rebuilt. This is useful if you want to remove
    some data because it takes up a lot of space, or, in the case of
    GDPR and privacy rules, you don't want to store any sensitive
    personal health data in your project. Use it like
    `tar_invalidate()`, with functions like `everything()` or
    `starts_with()`.

`tar_prune()`

:   This function is useful to help clean up left over or unused objects
    in the `_targets/` folder. You will probably not use this function
    too often.

`tar_destroy()`

:   The most destructive, and probably more commonly used, function.
    This will delete the entire `_targets/` folder for those times when
    you want to start over and re-run the entire pipeline again.

1.  Try out `tar_invalidate()` by running these code in the Console, one
    at a time, and see what output they give:

    ```{r test-out-invalidate}
    #| purl: true
    tar_read(lipidomics)
    tar_invalidate(everything())
    tar_read(lipidomics)
    tar_outdated()
    tar_make()
    tar_outdated()
    ```

2.  Try out `tar_delete()` by also running these code in the Console,
    one at a time, and see what output they give:

    ```{r test-out-delete}
    #| purl: true
    tar_read(lipidomics)
    tar_delete(everything())
    tar_read(lipidomics)
    tar_outdated()
    tar_make()
    tar_outdated()
    ```

While this isn't an issue for our data (since it is an open dataset),
it's good practice to not store the original, individual level data in
the `_targets/` folder for those of us working in more strict
environments when it comes to personal health data.

Open the `_targets.R` file and write this code at the end of the file:

```{r}
tar_delete(lipidomics)
```

```{r purl-only-delete-stored-data}
#| eval: false
#| echo: false
#| purl: true
"tar_delete(lipidomics)" |>
  readr::write_lines(here::here("_targets.R"), append = TRUE)
gert::git_add("_targets.R")
gert::git_commit("Delete the stored lipidomics data in the _targets folder")
```

Before ending, open the Git interface and commit the changes you made.
Then push your changes up to GitHub.

## Summary
