---
execute: 
  eval: false
---

# Creating automatic analysis pipelines {#sec-pipelines}

```{r setup}
#| include: false
#| eval: true
source(here::here("R/functions.R"))
library(tidyverse)
load(here::here("data/lipidomics.rda"))
```

{{< include ../includes/_wip.qmd >}}

::: {.callout-note appearance="minimal" collapse="true"}
## Instructor note

Before beginning, get them to recall what they remember of the previous
session, either with something like Mentimeter or verbally. Preferably
something like Mentimeter because it allows everyone to participate, not
just the ones who are more comfortable being vocal to the whole group.
:::

Depending on how much time you've spent working on data analyses, you
have probably experienced (many) times where you are working on a
project and forget what code needs to be run first, what order other
code needs to run in, and what pieces of code need to be re-run in order
to update other results. Things get confusing quickly, even for fairly
simple projects. This probably happens most often when you return to a
project after a month or two and completely forget the state of the
project and analysis.

This is where formal data analysis pipeline tools come in and play a
role. By setting up your analysis into distinct steps, with clear inputs
and outputs, and use a system that tracks those inputs and outputs, you
can make things a lot easier for yourself and others. This session is
about applying tools that make and manage these pipelines.

## Learning objectives

The overall objective for this session is to:

1.  Identify and apply an approach to creating an analysis pipeline that
    makes your analysis steps, from raw data to finished manuscript,
    explicitly defined so that updating it by either you or
    collaborators is as easy as running a single function.

More specific objectives are to:

1.  Describe the computational meaning of pipeline and how pipelines are
    often done in research. Explain why a well-designed pipeline can
    streamline your collaboration, reduce time spent doing an analysis,
    make your analysis steps explicit and easier to work on, and
    ultimately contribute to more fully reproducible research.
2.  Explain the difference between a "function-oriented" workflow vs a
    "script-oriented" workflow and why the function-based approach has
    multiple advantages from a time- and effort-efficiency point of
    view.
3.  Use the functions within `{targets}` to apply the concepts of
    building pipelines in your analysis project.
4.  Continue applying the concepts and functions used from the previous
    session.

## Exercise: How do you re-run analyses when something changes?

> Time: \~14 minutes.

We've all been in situations where something in our analysis needs to
change. Maybe we forgot to remove a certain condition (like unrealistic
BMI). Or maybe our supervisor suggests something we hadn't considered in
the analysis. Or maybe during peer review of our manuscript, a reviewer
makes a suggestion that would improve the understanding of the paper.
What ever the situation, we inevitably need to re-run our analysis. And
depending on what the change was, we might need to run the full analysis
all over again. So what is your **exact** workflow when you need to
re-run code and update your results? Assume it's a change somewhere
early in the data processing stage.

1.  Take 2 min to think about the workflow you use. Try to think of the
    *exact* steps you need to take, what *exactly* you do, and how long
    that usually takes.
2.  For 8 min, in your group share and discuss what you've thought. How
    do your experiences compare to each other?
3.  For the remaining time, we'll briefly share with everyone about what
    they've thought and discussed.

## What is a data analysis "pipeline"?

TODO: Should this be a reading exercise or the instructor talk about it?

A pipeline can be any process where the steps between a start and an end
point are very clear, explicit, and concrete. These highly distinct
steps can be manual, human involved or can be completely automated by a
robot or computer. For instance, in car factories, the pipeline from the
input raw materials to the output vehicle are extremely well described
and implemented. Or like during the pandemic, the pipeline for testing
(at least in Denmark and several other countries) was highly structured
and clear for the workers doing the testing and the people having the
test done: A person goes in, scans their ID card, has the test done,
worker inputs the results, results get sent immediately to the health
agency as well as to the person based on their ID contact information
(or via a secure app).

However, in research, especially around data collection and analysis, we
often hear or read about "pipelines". But looking closer, these aren't
actual pipelines because the individual steps are not very clear and not
well described, often requiring a fair amount of manual human attention
and intervention. Particularly within computational environments, a
pipeline is when there is minimal to *no* human intervention from raw
input to finished output. Why aren't these data "pipelines" in research
actual pipelines? Because:

1.  Anything with data ultimately must be on the computer,
2.  Anything automatically done on the computer must be done with code,
3.  Not all researchers write code,
4.  Researchers who do write code rarely publish and share it,
5.  Code that is shared or published (either publicly or within the
    research group) is not written in a way that allows a pipeline to
    exist,
6.  And, research is largely non-reproducible [@Trisovic2022;
    @Seibold2021; @Laurinavichyute2021].

A data analysis pipeline would by definition be a reproducible,
readable, and code-based data analysis. we researchers as a group have a
long way to go before we can start realistically implementing data
analysis pipelines.

This isn't to diminish the work of researchers, but a basic observation
on the systemic, social, and structural environment that we researchers
work in. We as researchers are not trained on how to write code, nor do
we have a strong culture and incentive structure around learning,
sharing, reviewing, and improving code. Nor are we often allowed to get
(or use) funds to hire people who *are* trained on and skilled in
programmatic thinking and programming. Otherwise courses like this
wouldn't need to exist :shrug:

So how would a data analysis pipeline look like? Before we get to that
though, we have to separate two things: exploratory data analysis and
final paper data analysis. In exploratory data analysis, there will
likely be a lot of manual, interactive steps involved that may or may
not need to be explicitly stated and included in the analysis plan and
pipeline. But for the final paper and what results would be included, we
generally have some basic first ideas. Let's list a few items that we
would want to do before the primary statistical analysis:

1.  A table of some basic descriptive statistics of the study
    population, such as mean, standard deviation, or counts of basic
    discrete data (like treatment group).
2.  A figure visualizing the counts or other statistics for variables of
    interest that are discrete/categorical. For the data in this course,
    that would be variables like gender and age.
3.  A figure showing the distribution of your main variables of
    interest. In this case, ours are the lipidomic variables.
4.  The paper with the results included.

```{mermaid fig-pipeline-schematic}
%%| label: fig-pipeline-schematic
%%| fig-cap: Very simple flow chart of steps in an analysis pipeline.
%%| echo: false
%%| eval: true
%%{init:{'theme':'forest', 'flowchart':{'nodeSpacing': 20, 'rankSpacing':30}}}%%
graph LR
    data[Data] --> fn_desc{{function}} 
    fn_desc --> tab_desc[Table:<br>descriptive<br>statistics]
    data --> fn_plot{{function}}
    fn_plot{{function}} --> plot_discr[Plot:<br>Discrete<br>variables]
    tab_desc --> paper[Paper]
    plot_discr --> paper
    data --> fn_plot_vars{{function}}
    fn_plot_vars{{function}} --> plot_distrib[Plot:<br>Continuous<br>variables]
    plot_distrib --> paper

linkStyle 0,1,2,3,4,5,6,7,8 stroke-width:1px;
```

Now that we conceptually have drawn out the sets of tasks to complete in
our pipeline, we can start using R to build it.

## Using targets to manage the pipeline

There are a few packages to help build pipelines in R, but the most
commonly used, well-designed, and maintained one is called `{targets}`.
With this package, you specify outputs you want to create and
`{targets}` will track them for you. So it will know which output
depends on which other one and which ones need to be updated.

::: {.callout-note appearance="minimal" collapse="true"}
## Instructor note

Ask participants, which do they think it is: a build dependency or a
workflow dependency. Because it is directly used to run analyses and
process the data, it would be a build dependency.
:::

First, since we are using `{renv}`, we need to install `{targets}` in
the project environment. And since `{targets}` is a build dependency, we
add it to the `DESCRIPTION` file with:

```{r add-targets-as-dep}
#| purl: true
use_package("targets")
```

After it finishes being added to the project R library, let's set up our
project to start using it!

```{r use-targets}
#| purl: true
targets::use_targets()
```

This will add several files:

```{r purl-only-use-targets-files}
#| eval: false
#| echo: false
#| purl: true
# TODO: Use to list out files created to paste into book.
fs::dir_ls(regexp = "run.*|_targets.*")
git_ci(
  c("run.R", "run.sh", "_targets.R"),
  "Start using targets"
)
```

The most important file is the `_targets.R` file. The other two files
are used for other situations (like running on a Linux server) that we
won't cover in the course. Before we continue though, let's **commit
these new files to the Git history**.

::: {.callout-note appearance="minimal" collapse="true"}
## Instructor note

Let them read it before going over it again to reinforce
function-oriented workflows and how `{targets}` and the `tar_target()`
works.
:::

::: callout-info
## Reading task: \~8

Next, open up the `_targets.R` and we'll take a look at what it
contains.

::: {.callout-note collapse="true"}
## Click this to see the file contents

```{r show-use-targets-file-contents}
#| eval: true
#| echo: false
#| code-fold: true
fs::path_package("targets", "pipelines", "use_targets.R") |>
  readLines() |>
  cat(sep = "\n")
```
:::

One small thing to notice are the `#nolint` comments used throughout.
These "tags" are used to tell `{lintr}` to ignore these lines when
running the lintr.

Next, notice `tar_target()` function used at the end of the script.
There are two main arguments for it `name` and `command`. The way that
`{targets}` works is similar to how you'd write R code to assign the
output of a function to an object.

``` r
object_name <- function_in_command(input_arguments)
```

Is the same as:

``` r
tar_target(
  name = object_name,
  command = function_in_command(input_arguments)
)
```

What this means is that `{targets}` follows a ["function-oriented"
workflow](https://books.ropensci.org/targets/functions.html#problems-with-script-based-workflows),
not a "script-based" workflow. What's the difference? In a
script-oriented workflow, each R file is run in a specific order, so you
might end up with an R file that has code like:

``` r
source("R/1-process-data.R")
source("R/2-basic-statistics.R")
source("R/3-create-plots.R")
source("R/4-linear-regression.R")
```

While in a function-oriented workflow, it might look more like:

``` r
source("R/functions.R")
raw_data <- load_raw_data("file/path/data.csv")
processed_data <- process_data(raw_data)
basic_stats <- calculate_basic_statistics(processed_data)
simple_plot <- create_plot(processed_data)
model_results <- run_linear_reg(processed_data)
```

With this workflow, each function takes an input dataset and contains
all the code to create the results into one output, like a figure in a
paper. If you've taken the [intermediate R
course](https://r-cubed-intermediate.rostools.org/), you'll notice that
this function-oriented workflow is the workflow we covered. There are so
many advantages to this type of workflow and is the reason many powerful
R packages are designed around making use of this type workflow.

If we take these same code and convert it into the `{targets}` format,
the end of `_targets.R` file would like this:

``` r
list(
  tar_target(
    name = raw_data,
    command = load_raw_data("file/path/data.csv")
  ),
  tar_target(
    name = processed_data,
    command = process_data(raw_data)
  ),
  tar_target(
    name = basic_stats,
    command = calculate_basic_statistics(processed_data)
  ),
  tar_target(
    name = simple_plot,
    command = create_plot(processed_data)
  ),
  tar_target(
    name = model_results,
    command = run_linear_reg(processed_data)
  )
)
```
:::

Let's start writing code to create the four items we listed above: some
descriptive statistics, a plot of some discrete data, a plot of the
continuous lipid variables, and a report (R Markdown). Since we'll use
`{tidyverse}`, specifically `{dplyr}`, to calculate the summary
statistics, we need to add it to our dependencies and install it in the
project library. `{tidyverse}` is a special "meta"-package so we need to
add it to the `"depends"` section of the `DESCRIPTION` file.

```{r add-tidyverse-deps}
#| purl: true
use_package("tidyverse", "depends")
use_package("dplyr")
```

Open up the `doc/lesson.Rmd` file and create a new header and code chunk
at the bottom of the file.

    ## Basic statistics

    ```{{r setup}}
    library(tidyverse)
    load(here::here("data/lipidomics.rda"))
    ```

    ```{{r basic-stats}}

    ```

```{r purl-only-markdown-basic-stats}
#| eval: false
#| echo: false
#| purl: true
c(
  "\n## Basic statistics\n\n```{r setup}\nlibrary(tidyverse)",
  "load(here::here('data/lipidomics.rda'))\n```\n\n"
) |>
  paste0(collapse = "\n") |>
  write_to_file("doc/lesson.Rmd")
git_ci("doc/lesson.Rmd", "Add section on basic stats to rmd")
```

TODO: Save this markdown text in order to add to project via purl?

We want to calculate the mean and SD for each metabolite and than, to
make it more readable, to round the numbers to one digit. We covered
this in the
[functionals](https://r-cubed-intermediate.rostools.org/dry-functionals.html#summarizing-long-data-like-the-rr-dataset)
session of the intermediate course, so we will apply these same
principles and code here. To do that, we need to use `group_by()` on the
`metabolites`, use `across()` inside `summarise()` so we can give it the
`mean()` and `sd()` functions, followed by `mutate()`ing each numeric
column (`across(where(is.numeric))`) to `round()` to 1 digit. Let's
write out the code!

```{r mean-sd-by-each-metabolite}
#| eval: true
lipidomics %>%
  group_by(metabolite) %>%
  summarise(across(value, list(mean = mean, sd = sd))) %>%
  mutate(across(where(is.numeric), round, digits = 1))
```

After that, run `r run_styler_text` on the file. Than we will **commit**
the changes to the Git history.

## Exercise: Convert summary statistics code into a function

> Time: \~25 minutes.

While inside the `doc/lesson.Rmd` file, use the "function-oriented"
workflow, as taught in the [intermediate
course](https://r-cubed-intermediate.rostools.org/dry-functions.html#the-basics-of-a-function),
to take the code we wrote above and convert it into a function. Complete
these tasks:

1.  Wrap the code with `function() {...}` and name the new function
    `descriptive_stats`.
2.  Replace `lipidomics` with `data` and put `data` as an argument
    inside the brackets of `function()`.
3.  Add `dplyr::` to the start of each `{dplyr}` function used inside
    your function (except for `where()`, which comes from the
    `{tidyselect}` package).
4.  Run `r run_styler_text` and `r run_lintr_text` on the code to make
    sure it is formatted correctly. You might need to manually force a
    styling if lines are too long.
5.  With the *cursor* inside the function, add some roxygen
    documentation with `Ctrl-Shift-P` followed by typing "roxygen
    comment". Remove the lines that contain `@examples` and `@export`,
    then fill in the other details (like the `@params` and `Title`). In
    the `@return` section, write "A data.frame/tibble."
6.  Cut and paste the function over into the `R/functions.R` file.
7.  Source the `R/functions.R` file (`Ctrl-Shift-S`) and then test the
    code by running `descriptive_stats(lipidomics)` in the Console. If
    it works, do the last task.
8.  Save both files and then open the Git interface and commit the
    changes you made to them.

Here is some scaffolding to help you get started:

``` r
descriptive_stats <- function(___) {
  ___
}
```

```{r solution-summary-stats-to-function}
#| eval: true
#| code-fold: true
#| code-summary: "**Click for a potential solution**. Only click if you are struggling or are out of time."
#' Calculate descriptive statistics of each metabolite.
#'
#' @param data Lipidomics dataset.
#'
#' @return A data.frame/tibble.
#'
descriptive_stats <- function(data) {
  data %>%
    dplyr::group_by(metabolite) %>%
    dplyr::summarise(dplyr::across(value, list(mean = mean, sd = sd))) %>%
    dplyr::mutate(dplyr::across(tidyselect::where(is.numeric), round, digits = 1))
}
```

```{r purl-only-summary-stats-function}
#| purl: true
#| echo: false
"
#' Calculate descriptive statistics of each metabolite.
#'
#' @param data Lipidomics dataset.
#'
#' @return A data.frame/tibble.
#'
descriptive_stats <- function(data) {
  data %>%
    dplyr::group_by(metabolite) %>%
    dplyr::summarise(dplyr::across(value, list(mean = mean, sd = sd))) %>%
    dplyr::mutate(dplyr::across(dplyr::where(is.numeric), round, digits = 1))
" |>
  write_to_file("R/functions.R")
git_ci("R/functions.R", "Add function to create descriptive stats.")
```

## Adding a step in the pipeline

Now that we've created a function to calculate some basic statistics, we
can now add it as a step in the `{targets}` pipeline. Open up the
`_targets.R` file and go to the end of the file, where the `list()` and
`tar_target()` code are found. In the first `tar_target()`, replace the
target to load the lipidomic data. In the second, replace it with the
`descriptive_stats()` function. If we want to make it easier to remember
what the target output is, we can add `df_` to remind us that it is a
data frame. It should look like:

``` r
list(
  tar_target(
    name = lipidomics,
    command = load(here::here("data/lipidomics.rda"))
  ),
  tar_target(
    name = df_stats_by_metabolite,
    command = descriptive_stats(lipidomics)
  )
)
```

```{r purl-only-add-to-targets}
#| eval: false
#| echo: false
#| purl: true
new_targets <- "
list(
  tar_target(
    name = lipidomics,
    command = load(here::here('data/lipidomics.rda'))
  ),
  tar_target(
    name = df_stats_by_metabolite,
    command = descriptive_stats(lipidomics)
  )
)
"
revise_by_line_num(
  path = "_targets.R",
  insert_text = new_targets,
  remove_original_lines = -28:-38
)
git_ci("_targets.R", "Add new targets to run.")
```

Let's run `{targets}` to see what happens! You can either use the
Command Palette (`Ctrl-Shift-P`, then type "run targets") or run this
code in the Console:

``` r
targets::tar_make()
```

It probably won't run though. That's because `{targets}` doesn't know
about the packages that you need for the pipeline. To add it, we need to
go to the `tar_option_set()` section of the `_targets.R` file and
replace the `packages = c("tibble")` with all the packages we use.
Instead of typing each package out, we can paste them directly into the
script using code, since we already track the packages in the
`DESCRIPTION` file. To access the dependencies, we can use this
function:

``` r
desc::desc_get_deps()$package
```

But this code also picks up the dependency on R itself. We can remove
that with `[-1]` and put it in the place of the `packages` argument:

``` r
packages = desc::desc_get_deps()$package[-1]
```

```{r purl-only-add-packages-to-targets}
#| eval: false
#| echo: false
#| purl: true
revise_by_text(
  path = "_targets.R",
  original = "packages = .*$",
  replacement = "packages = desc::desc_get_deps()$package[-1],"
)
```

Try running `{targets}` again with either `targets::tar_make()` or the
Command Palette (`Ctrl-Shift-P`, then type "run targets"). It should run
through! We also see that a new folder has been created called
`_targets/`. Since we don't want to track this in Git, we should ignore
it with:

```{r ignore-targets-folder}
#| purl: true
use_git_ignore("_targets")
```

We can visualize our individual pipeline targets that we track through
`tar_target()` now too, which can be useful as you add more and more
targets:

```{r visualize-targets}
#| purl: true
targets::tar_visnetwork()
```

Or to see what pipeline targets are outdated:

```{r outdated-targets}
#| purl: true
targets::tar_outdated()
```

Before continuing, let's **commit the changes to the Git history**.

```{r purl-only-commit-packages-to-targets}
#| eval: false
#| echo: false
#| purl: true
git_ci(c(".gitignore", "_targets.R"), "Tell targets about packages, ignore _targets/")
```

## Exercise: Examine the contents of the `_targets/` folder

> Time: \~5 minutes.

Like you did with `{renv}`, go into the `_targets/` folder and take a
look around. There are two important ones: `meta/` and `objects/`. Look
in each to get familiar with the files there.

-   The `meta/meta` file contains the metadata about the individual
    pipeline targets that you add with `tar_target()`.
-   The `objects/` folder contains the output of each `tar_target()`
    (e.g. the data from `load()` or the statistics from
    `descriptive_stats()`).

TODO: Need to run on my end to know what to look for.

## Exercise: Update function to also calculate median and IQR

> Time: \~8 minutes.

Let's make a change to our function and test out how the
`tar_outdated()` and `tar_visnetwork()` work.

1.  Open up the `R/functions.R` file.
2.  Add median and interquartile range (IQR) to the `summarise()`
    function, by adding it to the end of `list(mean = mean, sd = sd)`,
    after the second `sd`. Note, IQR should look like `iqr = IQR` since
    we want the output columns to have a lowercase for the column names.
3.  Run `tar_outdated()` and `tar_visnetwork()` in the Console (or by
    using the Command Palette `Ctrl-Shift-P`, then "targets outdated" or
    "targets visualize"). What does it show?
4.  Run `r run_styler_text` and `r run_lintr_text`. You might need to
    force a reformat if the code is too long by highlighting the line
    and using `Ctrl-Shift-P`, then "reformat".
5.  Run `tar_make()` in the Console (or `Ctrl-Shift-P`, then "targets
    run", selecting the "background" option). Re-check for outdated
    targets and visualize the network again.
6.  Open up the Git interface and commit the changes to the Git history.

```{r solution-more-summary-stats-to-function}
#| code-fold: true
#| code-summary: "**Click for a potential solution**. Only click if you are struggling or are out of time."
#' Calculate descriptive statistics of each metabolite.
#'
#' @param data Lipidomics dataset.
#'
#' @return A data.frame/tibble.
#'
descriptive_stats <- function(data) {
  data %>%
    dplyr::group_by(metabolite) %>%
    dplyr::summarise(dplyr::across(value, list(
      mean = mean,
      sd = sd,
      median = median,
      iqr = IQR
    ))) %>%
    dplyr::mutate(dplyr::across(dplyr::where(is.numeric), round, digits = 1))
}
```

```{r purl-only-more-summary-stats-to-function}
#| eval: false
#| echo: false
#| purl: true
"
#' Calculate descriptive statistics of each metabolite.
#'
#' @param data Lipidomics dataset.
#'
#' @return A data.frame/tibble.
#'
descriptive_stats <- function(data) {
  data %>%
    dplyr::group_by(metabolite) %>%
    dplyr::summarise(dplyr::across(value, list(
      mean = mean,
      sd = sd,
      median = median,
      iqr = IQR
    ))) %>%
    dplyr::mutate(dplyr::across(dplyr::where(is.numeric), round, digits = 1))
}
" |>
  write_to_file("R/functions.R")
```

## Saving intermediate analysis outputs

Sometimes you may want a pipeline target to save the output to a file so
you can save it to the Git history, like saving an figure, rather than
store the result in the `_targets/` folder. To save outputs of a target
outside of the `_targets/` folder, there are a few specific things we
have to do.

First, let's write some code to create the plots for our items 2 and 3
that we initially decided at the start of this session. Since we're
using `{ggplot2}` to write this code, let's add it to our `DESCRIPTION`
file and to our `{renv}` project library.

```{r add-ggplot2-deps}
#| purl: true
use_package("ggplot2")
```

Next, we'll write the code to create item 2: The bar plot of the counts
of `gender` and `class`. We'll keep it simple for now. Because the data
is structured in a long format, we need to trim it down to only the
unique cases of `code`, `gender`, and `class` with `distinct()`. Then we
can make the bar plot. We'll use `position = "dodge"` for the bars to be
side by side, rather than stacked.

```{r bar-plot-gender-class}
#| eval: true
gender_by_class_plot <- lipidomics %>%
  distinct(code, gender, class) %>%
  ggplot(aes(x = class, fill = gender)) +
  geom_bar(position = "dodge")
gender_by_class_plot
```

Since we want to save this plot to a file, we'll use `ggsave()` to save
it to a new folder called `images/`.

```{r bar-plot-gender-class-to-file}
ggsave(here::here("images/gender-by-class.png"), gender_by_class_plot)
```

Next, let's do item 3, the distribution of each metabolite. Here we'll
use `geom_histogram()`, nothing too fancy. And since the data is already
in long format, we can easily use `facet_wrap()` to create a plot for
each metabolite. Like with the plot above, let's save it to a file. We
use `scales = "free"` because each metabolite doesn't have the same
range of values (some are small, others are quite large).

```{r histogram-metabolites}
metabolite_distribution_plot <- ggplot(lipidomics, aes(x = value)) +
  geom_histogram() +
  facet_wrap(vars(metabolite), scales = "free")
metabolite_distribution_plot
ggsave(
  here::here("images/metabolite-distribution.png"),
  metabolite_distribution_plot
)
```

We now have the basic code to convert over into functions. There are
some special things `{targets}` needs in order for it to recognize that
you want to save the output as a file. Before we do that though, we'll
create the functions to start with.

## Exercise: Convert the plot code to a function

> Time: \~20 minutes.

For now, we will only take the code to make the bar plot and convert it
into a function. Just like you did with the `descriptive_stats()`
function in the exercise above, complete these tasks:

1.  Wrap the code with `function() {...}` and name the new function
    `plot_count_stats`.
2.  Replace `lipidomics` with `data` and put `data` as an argument
    inside the brackets of `function()`.
3.  Replace the `"images/gender-by-class.png"` with `image_path` and put
    `image_path` as an argument inside the brackets of `function()`.
4.  Add `ggplot2::` and `dplyr::` to the start of each `{ggplot2}` and
    `{dplyr}` function used inside your function.
5.  Run `r run_styler_text` and `r run_lintr_text` on the code to make
    sure it is formatted correctly. You might need to manually force a
    styling if lines are too long.
6.  With the *cursor* inside the function, add some roxygen
    documentation with `Ctrl-Shift-P` followed by typing "roxygen
    comment". Remove the lines that contain `@examples` and `@export`,
    then fill in the other details (like the `@params` and `Title`). In
    the `@return` section, write "A plot object or a file path."
7.  Cut and paste the function over into the `R/functions.R` file.
8.  Source the `R/functions.R` file (`Ctrl-Shift-S`) and then test the
    code by running `plot_count_stats(lipidomics)` in the Console. If it
    works, do the last task.
9.  Save both files and then open the Git interface and commit the
    changes you made to them.
10. Repeat these tasks for the code for the metabolite distribution
    plot. Call this new function `plot_distributions`.

Use this scaffolding code to help guide you to write the code into a
function.

``` r
plot_count_stats <- function(___, ___) {
  plot_output <- ___
  
  ggplot2::ggsave(
    plot = plot_output, 
    filename = here::here(___)
  )
  plot_output
}

plot_distributions <- function(___, ___) {
  plot_output <- ___
  
  ggplot2::ggsave(
    plot = plot_output, 
    filename = here::here(___)
  )
  plot_output
}
```

```{r solution-new-function-plots}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
## This should be in the R/functions.R file.
#' Plot for basic count data.
#'
#' @param data The lipidomics dataset.
#' @param image_path The file path location where to save the image.
#'
#' @return A ggplot2 graph.
#'
plot_count_stats <- function(data, image_path) {
  plot_output <- data %>%
    dplyr::distinct(code, gender, class) %>%
    ggplot2::ggplot(ggplot2::aes(x = class, fill = gender)) +
    ggplot2::geom_bar(position = "dodge")
  ggplot2::ggsave(
    plot = plot_output,
    filename = here::here(image_path)
  )
  plot_output
}

#' Plot for basic distribution of metabolite data.
#'
#' @param data The lipidomics dataset.
#' @param image_path The file path location where to save the image.
#'
#' @return A ggplot2 graph.
#'
plot_distributions <- function(data, image_path) {
  plot_output <- data %>% 
    ggplot2::ggplot(ggplot2::aes(x = value)) +
    ggplot2::geom_histogram() +
    ggplot2::facet_wrap(ggplot2::vars(metabolite), scales = "free")
  ggplot2::ggsave(
    plot = plot_output,
    filename = here::here(image_path)
  )
  plot_output
}
```

## Informing targets of the saved output files

We now have the functions that we can put into the `_targets.R` file.
Now we need to change the functions slightly in order for `{targets}` to
know about the saved images. When we use the functions inside
`tar_target()`, we need to do two things: have the function output the
file path of the image rather than output the plot; and, set the
argument `format` in `tar_target()` to `"file"`.

To do the first, a simple fix would be to go into the plot functions and
replace the `plot_output` at the end with `image_path`. But! That would
mean that when we are testing out the function, we couldn't ever
actually *see* the plot being created. Instead, we can create an
`if () ... else()` condition to output the plot when we are working
`interactive()`ly, otherwise output the image path when `{targets}` runs
it.

Since we'll be using this code at least twice, let's create another
function to do this switch for us. Open up the `R/functions.R` file and
start typing out the function:

```{r output-switch}
output_switch_for_targets <- function(output, path) {
  if (interactive()) {
    output
  } else {
    path
  }
}
```

After we've written it, we'll add some Roxygen documentation
(`Ctrl-Shift-P`, then type "roxygen comment"):

```{r new-function-output-switch-for-targets}
#' Switch output based on if it is interactive, otherwise it is a file path.
#'
#' @param output Output object to send to screen when running interactively.
#' @param path The file path to where the intermediate object is saved.
#'
#' @return An object or a file path.
#'
output_switch_for_targets <- function(output, path) {
  if (interactive()) {
    output
  } else {
    path
  }
}
```

Then, we go into the two plot functions and replace `plot_output` at the
end with:

``` r
output_switch_for_targets(plot_output, image_path)
```

TODO: Check purl'ed code to see which line to replace with.

Now, let's add the two functions to the `_targets.R` file, including
adding the `format = "file"` argument. Let's write these two
`tar_target()` items within the `list()` inside `_targets.R`. To make it
easier to track things, keep the file name and target `name` the same
(except maybe add `fig_` at the start of the name).

``` r
list(
  ...,
  tar_target(
    name = fig_gender_by_class,
    command = plot_count_stats(
      lipidomics, 
      "images/gender-by-class.png"
    ),
    format = "file"
  ),
  tar_target(
    name = fig_metabolite_distribution,
    command = plot_count_stats(
      lipidomics, 
      "images/metabolite-distribution.png"
    ),
    format = "file"
  )
)
```

Next, test that it works by running `targets::tar_visnetwork()` (or
`Ctrl-Shift-P`, then type "target visual") or `targets::tar_outdated()`.
You should see that the new items are "outdated". Then run
`targets::tar_make()` (`Ctrl-Shift-P`, then "targets run") to update the
pipeline. If it all works, than **commit the changes to the Git
history**.

```{r purl-only-plot-fns-in-targets}
#| eval: false
#| echo: false
#| purl: true
# TODO: need to update these numbers.
revise_by_text(
  path = "R/functions.R",
  original = "plot_output$",
  replacement = "output_switch_for_targets(plot_output, image_path)"
)
styler::style_dir()

update_targets_plots <- '
),
tar_target(
  name = fig_gender_by_class,
  command = plot_count_stats(lipidomics, "images/gender-by-class.png"),
  format = "file"
),
tar_target(
  name = fig_metabolite_distribution,
  command = plot_count_stats(lipidomics, "images/metabolite-distribution.png"),
  format = "file"
)
'
# -20 to remove the previous `)`
revise_by_line_num(
  path = "_targets.R",
  insert_text = update_targets_plots,
  remove_original_lines = -20,
  insert_at_line = 21
)
targets::tar_visnetwork()
targets::tar_make()
git_ci("R/functions.R", "Add output switch function, to use for targets")
git_ci("_targets.R", "Update targets with plot items")
```

## Incorporating targets with R Markdown

Last, but not least, we want to make our final item: The R Markdown
document. Adding an R Markdown document as a target inside `_targets.R`
is fairly straightforward. We need to install the helper package
`{tarchetypes}` first:

```{r tarchetypes-deps}
#| purl: true
use_package("tarchetypes")
```

Then, inside `_targets.R`, uncomment the line where
`library(tarchetypes)` is commented out. The function we need to use to
build the R Markdown file is `tar_render()`, which needs two things: The
`name`, like `tar_target()` needs, and the file path to the R Markdown
file. Again, like the other `tar_target()` items, add it to the end of
the `list()`:

``` r
list(
  ...,
  tar_render(
    name = lesson_rmd, 
    path = "doc/lesson.Rmd"
  )
)
```

TODO: Try replacing with `tar_quarto()` instead.

Now when we run `r run_tar_make_text`, the R Markdown file also gets
re-built. But when we use `targets::tar_visnetwork()`, we don't see the
connections with plots and descriptive statistics. That's because we
haven't used them in a way `{targets}` can recognize. For that, we need
to use the function `targets::tar_read()`.

TODO: Confirm that we don't see this.

Let's open up the `doc/lesson.Rmd` file and create a new header and code
chunk and make use of the `targets::tar_read()`

    ## Results

    ```{{r}}
    targets::tar_read(df_stats_by_metabolite)
    ```

    ```{{r}}
    knitr::include_graphics(targets::tar_read(fig_gender_by_class))
    ```

    ```{{r}}
    knitr::include_graphics(targets::tar_read(fig_distribution_metabolites))
    ```

TODO: Purl this markdown text

With `targets::tar_read()`, we can access all the stored target items
using syntax like we would with `{dplyr}`, without quotes. For the
`df_stats_by_metabolite`, we can do some minor wrangling with `mutate()`
and `glue::glue()`, and than pipe it to `knitr::kable()` to create a
table in the output document. The `{glue}` package is really handy for
formatting text based on columns. If you use `{}` inside a quoted
string, you can use columns from a data frame, like `value_mean`. So we
can use it to format the final table text to be `mean value (SD value)`:

```{r stats-to-table}
targets::tar_read(df_stats_by_metabolite) %>% 
  mutate(MeanSD = glue::glue("{value_mean} ({value_sd})")) %>%
  select(Metabolite = metabolite, `Mean SD` = MeanSD) %>%
  knitr::kable(caption = "Descriptive statistics of the metabolites.")
```

```{r purl-only-stats-to-table}
#| eval: false
#| echo: false
#| purl: true
pretty_basic_stats_code <- '
targets::tar_read(df_stats_by_metabolite) %>% 
  mutate(MeanSD = glue::glue("{value_mean} ({value_sd})")) %>%
  select(Metabolite = metabolite, `Mean SD` = MeanSD) %>%
  knitr::kable(caption = "Descriptive statistics of the metabolites.")
'
revise_by_text(
  path = "doc/lesson.Rmd",
  original = "targets::tar_read(df_stats_by_metabolite)",
  replacement = pretty_basic_stats_code
)
styler::style_file("doc/lesson.Rmd")
git_ci("doc/lesson.Rmd", "Basic stats as a pretty table.")
```

```{r execute-only-table-basic-stats}
#| eval: true
#| echo: false
lipidomics %>% 
  descriptive_stats() %>% 
  mutate(MeanSD = glue::glue("{value_mean} ({value_sd})")) %>%
  select(Metabolite = metabolite, `Mean SD` = MeanSD) %>%
  knitr::kable(caption = "Descriptive statistics of the metabolites.")
```

Re-run `r run_tar_vis_text` to see that it now detects the connections
between the pipeline targets. Now, run `r run_tar_make_text` again to
see everything re-build! Last things are to re-style
`r run_styler_text`, then **commit** the changes to the Git history
before moving on.

## Exercise: Removing the stored pipeline data

> Time: \~15 minutes.

Like with `{renv}`, sometimes you need to start from the beginning and
clean everything up because there's an issue that you can't seem to fix.
In this case, `{targets}` has a few functions to help out. Here are four
that you can use to delete stuff (also [described on the targets
book](https://books.ropensci.org/targets/data.html#cleaning-up-local-internal-data-files)):

`tar_invalidate()`

:   This removes the metadata on the target in the pipeline, but doesn't
    remove the object itself (which `tar_delete()` does). This will tell
    `{targets}` that the target is out of date, since it has been
    removed, even though the data object itself isn't present. You can
    use this like you would `select()`, by naming the objects directly
    or using the `{tidyselect}` helpers (e.g. `everything()`,
    `starts_with()`).

`tar_delete()`

:   This deletes the stored objects (e.g. the `lipidomics` or
    `df_stats_by_metabolite`) inside `_targets/`, but does not delete
    the record in the pipeline. So `{targets}` will see that the
    pipeline doesn't need to be rebuilt. This is useful if you want to
    remove some data because it takes up a lot of space, or, in the case
    of GDPR and privacy rules, you don't want to store any sensitive
    personal health data in your project. Use it like
    `tar_invalidate()`, with functions like `everything()` or
    `starts_with()`.

`tar_prune()`

:   This function is useful to help clean up left over or unused objects
    in the `_targets/` folder. You will probably not use this function
    too often.

`tar_destroy()`

:   The most destructive, and probably more commonly used, function.
    This will delete the entire `_targets/` folder for those times when
    you want to start over and re-run the entire pipeline again.

1.  Try out `tar_invalidate()` by running these code in the Console, one
    at a time, and see what output they give:

    ```{r test-out-invalidate}
    #| purl: true
    tar_read(lipidomics)
    tar_invalidate(everything())
    tar_read(lipidomics)
    tar_outdated()
    tar_make()
    tar_outdated()
    ```

2.  Try out `tar_delete()` by also running these code in the Console,
    one at a time, and see what output they give:

    ```{r test-out-delete}
    #| purl: true
    tar_read(lipidomics)
    tar_delete(everything())
    tar_read(lipidomics)
    tar_outdated()
    tar_make()
    tar_outdated()
    ```

While this isn't an issue for our data (since it is an open dataset),
it's good practice to not store the original, individual level data in
the `_targets/` folder for those of us working in more strict
environments when it comes to personal health data.

Open the `_targets.R` file and write this code at the end of the file:

```{r tar-delete-data}
tar_delete(lipidomics)
```

```{r purl-only-delete-stored-data}
#| eval: false
#| echo: false
#| purl: true
"tar_delete(lipidomics)" |>
  write_to_file("_targets.R", append = TRUE)
git_ci("_targets.R", "Delete the stored lipidomics data in the _targets folder")
```

Before ending, open the Git interface and commit the changes you made.
Then push your changes up to GitHub.

## Summary

-   Use a function-oriented workflow together with `{targets}` to build
    your data analysis pipeline and track your "pipeline targets".
-   List individual "pipeline targets" using `tar_target()` within the
    `_targets.R` file.
-   Visualize target items in your pipeline with
    `targets::tar_visnetwork()` or list outdated items with
    `targets::tar_outdated()`.
-   When saving output as a file, ensure that your function outputs the
    path to the file, *not* the output itself. Then use
    `format = "file"` as an argument in `tar_target()`.
-   Within R Markdown files, use `targets::tar_read()` to access saved
    pipeline outputs. To include the R Markdown in the pipeline, use
    `{tarchetypes}` and the function `tar_render()`.
-   Delete stored pipeline output with `tar_delete()`.
